{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import Imputer\n",
    "from fancyimpute import SoftImpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Output Variables\n",
    "\n",
    "This file predicts output variables using regression and classification models. Both regression are classification are applied to full dataset and clusters.\n",
    "\n",
    "Select input and output variables in the first section and then run functions at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Inputs and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We only use one output at a time. Here are the list of outputs to plug into the output variable below:\n",
    "\n",
    "* asset_owned___output\n",
    "* crop_diversification___output\n",
    "* crop_sales___output\n",
    "* crop_sales_specialization___output\n",
    "* expenditure___output\n",
    "* food_expenditure_diversification___output\n",
    "* income___output\n",
    "* income_diversification___output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, silhouette_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "from scipy import stats\n",
    "import glmnet_python\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def get_classes(output_var, pred):\n",
    "    max_bins = 3\n",
    "    _, boundaries = np.histogram(output_var, bins=max_bins)\n",
    "    classes = np.digitize(pred, bins=boundaries)\n",
    "    return classes, max_bins\n",
    "\n",
    "def for_year(var, year):\n",
    "    return var + '___ethiopia_' + str(year)\n",
    "\n",
    "colors= {2011:'r', 2013:'g', 2015:'blue'}\n",
    "ax = {}\n",
    "def run_regressions(in_name, out_dir, non_policy_inputs, segment_variables, inputs, output, year):\n",
    "    global table\n",
    "    global coef_table\n",
    "    global avg_table\n",
    "    global coef_map\n",
    "    # table for regressions and classification\n",
    "    table = pd.DataFrame()\n",
    "    avg_table = pd.DataFrame()\n",
    "    coef_map = {}\n",
    "    # create table of coefficients\n",
    "    coef_table = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(out_dir)\n",
    "    except:\n",
    "        print(\"Dir exists\")\n",
    "    \n",
    "    ols = linear_model.LinearRegression()\n",
    "    ridge = linear_model.Ridge(alpha=.5)\n",
    "    lasso = linear_model.Lasso(alpha = 0.01, max_iter=1e5)\n",
    "    lars_lasso = linear_model.LassoLars(alpha=.1)\n",
    "    bayes_ridge = linear_model.BayesianRidge()\n",
    "    sgd = linear_model.SGDRegressor()\n",
    "    svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "    svr_lin = SVR(kernel='linear', C=1e3)\n",
    "    svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "    kernel_ridge = KernelRidge(alpha=1.0)\n",
    "    \n",
    "    # pick which regression algos to use\n",
    "    regression_algorithms = (\n",
    "#        ('OrdinaryLeastSquares', ols),\n",
    "#        ('RidgeRegression', ridge),\n",
    "        ('Lasso', lasso),\n",
    "#         ('LARS Lasso', lars_lasso),\n",
    "#         ('BayesianRidgeRegression', bayes_ridge),\n",
    "#         ('StochasticGradientDescent', sgd),\n",
    "#         ('SupportVectorRegressionRBF', svr_rbf),\n",
    "#         ('SupportVectorRegressionLinear', svr_lin),\n",
    "#         ('SupportVectorRegressionPolynomial', svr_poly),\n",
    "#         ('KernelRidgeRegression', kernel_ridge)\n",
    "    )\n",
    "    \n",
    "    df = read_csv(in_name)\n",
    "    raw_df = read_csv(in_name.replace('normed', 'raw'))\n",
    "#     df[output] = raw_df[for_year('crop_sales___output', year)]/raw_df[for_year('land_surface', year)]*10000.0\n",
    "    df = df.loc[df[output].dropna().index] # drop rows with unobserved income\n",
    "#     df = df.loc[df['household_head_is_male___ethiopia_2015']==0]\n",
    "    df['nid']= df.index.tolist()\n",
    "#     filter_var = 'lives_in_amhara___ethiopia_' + str(year)\n",
    "#     df = df[df[filter_var]==True]\n",
    "    #df = df.loc[df[output] != 0] # drop zero outputs\n",
    "    # select % of data in test set\n",
    "    test_split = 0.2\n",
    "    \n",
    "    # perform matrix completion. completed is returned as a np array\n",
    "    # we've discussed not using it, but I left it in because I wasn't able to\n",
    "    # fit the StandardScaler with a DataFrame that contained NaN's\n",
    "\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "\n",
    "    # if we want to use low rank matrix completion inst\n",
    "    # completed = SoftImpute().complete(x)\n",
    "    \n",
    "    # reconstruct dataframe with completed matrix\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    \n",
    "#     raw_df[output] = raw_df[for_year('crop_sales___output', year)]/raw_df[for_year('land_surface', year)]*10000.0\n",
    "    raw_df = raw_df.loc[df[output].dropna().index]\n",
    "#     raw_df = raw_df.loc[raw_df['household_head_is_male___ethiopia_2015']==0]\n",
    "    raw_df['nid']= raw_df.index.tolist()\n",
    "    raw_imp = Imputer(strategy=\"mean\")\n",
    "    raw_completed = raw_imp.fit_transform(raw_df)\n",
    "    raw_cols = raw_df.columns.values\n",
    "    raw_df = pd.DataFrame(raw_completed,columns=raw_cols)\n",
    "    raw_df['productivity'] = raw_df[for_year('crop_sales___output', 2015)]/raw_df[for_year('land_surface', 2015)]\n",
    "    raw_df['productivity'] = raw_df['productivity'].apply(lambda x: 0 if x == np.inf else x)\n",
    "\n",
    "#     raw_df[output] = raw_df[output].apply(lambda x: 100.0*math.exp(x))\n",
    "    #relevant_vars = ['hired_labor___policy', 'oxen_owned___policy', 'chemical_fertilizers_used___policy', 'land_surface', 'plough_owned___policy']\n",
    "#     relevant_vars = list(segment_variables.keys())\n",
    "#     for seg_var in relevant_vars:\n",
    "# #         seg_var_year = for_year(seg_var, year)\n",
    "#         seg_var_year = seg_var\n",
    "#         raw_df[seg_var_year] = raw_df[seg_var_year].apply(lambda x: math.exp(x) - 1)\n",
    "#     weight_cols = filter(lambda x: x.find('weight') != -1, mat.columns)\n",
    "#     for col in cols:\n",
    "#         if col in weight_cols:\n",
    "#             continue\n",
    "#         mat[col] = mat[col] * mat[for_year('weight', year)]\n",
    "#     mat[output] = mat[output] * raw_df[for_year('weight', year)]\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "#     mat = mat.drop(weight_cols, axis=1)\n",
    "    #scale_up_factor = 100.0\n",
    "    #mat[output] = mat[output].apply(lambda x: x*scale_up_factor)\n",
    "    y = mat[output]\n",
    "    x = mat[inputs]\n",
    "    \n",
    "    def update_best_lambda(x_scaled):\n",
    "        global regression_algorithms\n",
    "        copy_y = np.array(y, dtype=np.float64)\n",
    "        print (copy_y)\n",
    "        fit = cvglmnet(x = x_scaled.copy(), y = copy_y)\n",
    "        regression_algorithms = (('Lasso', linear_model.Lasso(alpha=fit['lambda_min'], max_iter=1e5)))\n",
    "        return fit['lambda_min']\n",
    "    \n",
    "    # Split test/train\n",
    "    indices = range(len(mat))\n",
    "    x_train, x_test, y_train, y_test, ind_train, ind_test = \\\n",
    "        train_test_split(x, y, indices, test_size=test_split, random_state=42)\n",
    "    \n",
    "    def get_train_test(input_vars):\n",
    "        x = mat[input_vars].copy()\n",
    "#         for col in x.columns.values:\n",
    "#             x[col] = x[col] * raw_df[for_year('weight', year)]\n",
    "        x_scaled = StandardScaler()\n",
    "        x_scaled.fit(x)\n",
    "        x_sc = x_scaled.transform(x)\n",
    "        # reconstruct DataFrame\n",
    "        x = pd.DataFrame(x_sc, columns=x.columns)\n",
    "        training_x = x.iloc[ind_train, :]\n",
    "        testing_x = x.iloc[ind_test, :]\n",
    "        return x_sc, training_x, testing_x\n",
    "    \n",
    "    def digitize(output_var, pred):\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        classes, max_bins = get_classes(output_var, pred)\n",
    "        b_classes = label_binarize(classes, range(max_bins))\n",
    "        return b_classes\n",
    "    \n",
    "    def calc_unsegmented(baseline, x_train, x_test): \n",
    "        global table\n",
    "        global coef_map\n",
    "        # reg keeps predictions from regressions along with keys\n",
    "        reg = dict()\n",
    "        for name, algo in regression_algorithms:\n",
    "            reg[name] = {}\n",
    "\n",
    "        # keys and values from test data\n",
    "        keys_list = []\n",
    "        y_list = []\n",
    "        for k, v in y_test.iteritems():\n",
    "            keys_list.append(k)\n",
    "            y_list.append(v)\n",
    "\n",
    "        # run regressions on full dataset\n",
    "        for name, algo in regression_algorithms:\n",
    "            model = sm.OLS(y_train, x_train)\n",
    "            fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "            \n",
    "#             model = algo.fit(x_train,y_train)\n",
    "            print(\"Confidence intervals are {0}\".format(fit.conf_int()))\n",
    "            y_pred = fit.predict(x_test)\n",
    "#             y_pred = model.predict(x_test)\n",
    "\n",
    "            # add predictions to dict\n",
    "            for i, p in enumerate(y_pred):\n",
    "                t = reg[name]\n",
    "                t[keys_list[i]] = p\n",
    "\n",
    "            try:\n",
    "                test_c = digitize(y, y_test)\n",
    "                pred_c = digitize(y, y_pred)\n",
    "                auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "            except ValueError:\n",
    "                auc_c = 0.5\n",
    "            mse = mean_squared_error(y_test,y_pred)\n",
    "            scaled_mse = (mse/np.std(y))\n",
    "\n",
    "            # add row to table\n",
    "            new_row = pd.DataFrame({'model': name, 'segment': '', 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': False}, index=[0])\n",
    "            table = table.append(new_row, ignore_index=True)\n",
    "\n",
    "            # add coefficients to map\n",
    "#             coef_map[name + '_' + baseline] = model.coef_\n",
    "            coef_map[name + '_' + baseline] = fit.params\n",
    "            lower_bounds = []\n",
    "            upper_bounds = []\n",
    "            for ci in fit.conf_int():\n",
    "                lower_bounds += [ci[0]]\n",
    "                upper_bounds += [ci[1]]\n",
    "            coef_map[name + '_' + baseline + '_lower_bound'] = lower_bounds\n",
    "            coef_map[name + '_' + baseline + '_upper_bound'] = upper_bounds\n",
    "    \n",
    "    def calc_segmented(segment_variables, baseline, x_train, x_test):\n",
    "        global table\n",
    "        global coef_map\n",
    "        global avg_table\n",
    "#         segment_vars = segment_variables\n",
    "        segment_vars = list(segment_variables.keys())\n",
    "        def iterative_clustering(sc_thres, corr_thres, alpha):\n",
    "            max_sc = 0\n",
    "            best_kmeans = None\n",
    "            \n",
    "#                 alpha = (1+a)/10.0\n",
    "            sc_incr = 1e-8\n",
    "            sc_thres = -1e-1\n",
    "            C = set([])\n",
    "            avail = set(segment_variables.keys())\n",
    "            prev_sc = 0\n",
    "            min_k = 4\n",
    "            corr_thres = 0.05\n",
    "            while sc_incr >= sc_thres:\n",
    "                u_f = 0\n",
    "                best_f = None\n",
    "                sc_best = 0\n",
    "                for f in avail:\n",
    "                    if segment_variables[f] < corr_thres:\n",
    "                        continue\n",
    "                    seg_data = mat[list(avail) + [f]]\n",
    "                    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "                    labels = kmeans.labels_\n",
    "                    sc = silhouette_score(seg_data, labels)\n",
    "                    sc_delta = sc - prev_sc\n",
    "                    if u_f < (segment_variables[f] + (alpha)*sc_delta):\n",
    "                        u_f = (segment_variables[f] + (alpha)*sc_delta)\n",
    "                        best_f = f\n",
    "                        sc_best = sc\n",
    "                if best_f == None:\n",
    "                    break\n",
    "                avail.remove(best_f)\n",
    "                C.add(best_f)\n",
    "#                 print (alpha, best_f, u_f, sc, prev_sc)\n",
    "                sc_incr = sc - prev_sc\n",
    "                prev_sc = sc\n",
    "            \n",
    "            seg_data = mat[list(C)]\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "            labels = kmeans.labels_\n",
    "            sc = silhouette_score(seg_data, labels)\n",
    "            return sc, kmeans\n",
    "\n",
    "        \n",
    "        def add_clusters():\n",
    "#             from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "            # elbow method\n",
    "            sse = []\n",
    "        \n",
    "                \n",
    "#             #### Multiply by correlation coefficient\n",
    "            seg_data = mat[segment_vars]\n",
    "            for seg_var in segment_vars:\n",
    "#                 seg_data[seg_var] = seg_data[seg_var]*mat[for_year('weight', year)]\n",
    "                seg_data[seg_var] = seg_data[seg_var].apply(lambda x: x*segment_variables[seg_var])\n",
    "    \n",
    "            ### Plot elbow\n",
    "#             for k in range(1,9):\n",
    "#                 kmeans = KMeans(n_clusters=k).fit(seg_data)\n",
    "#                 labels = kmeans.labels_\n",
    "#                 sse.append(sum(np.min(cdist(seg_data, kmeans.cluster_centers_, 'euclidean'), axis=1)) / seg_data.shape[0])\n",
    "\n",
    "#             plt.clf()\n",
    "#             plt.plot(range(1,9), sse)\n",
    "#             plt.xlabel('k')\n",
    "#             plt.ylabel('Sum of squared error')\n",
    "#             plt.savefig(os.path.join(out_dir, 'elbow.png'))\n",
    "#             print(sse)\n",
    "#             min_k = sse.index(min(sse))\n",
    "#             print (min_k)\n",
    "\n",
    "            ### Iterative clustering\n",
    "#             max_sc = -1\n",
    "#             best_kmeans = None\n",
    "#             for sc_thres in [-0.1, -0.05, -0.01, -0.001, -1e-3, 0, 1e-3, 1e-2]:\n",
    "#                 for corr_thres in [0.05, 0.06, 0.08, 0.1, 0.12, 0.15]:\n",
    "#                     for alpha in [1e-4,1e-3,1e-2,0.1,1,10,100,1000,10000]:\n",
    "#                         sc, kmeans = iterative_clustering(sc_thres, corr_thres, alpha)\n",
    "#                         if sc > max_sc:\n",
    "#                             max_sc = sc\n",
    "#                             best_kmeans = kmeans\n",
    "#                             print ('Best found: {0}, {1}, {2}'.format(sc_thres, corr_thres, alpha))\n",
    "            \n",
    "#             kmeans = best_kmeans\n",
    "    \n",
    "            min_k = 4\n",
    "            kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "            labels = kmeans.labels_\n",
    "            mat['cluster'] = labels\n",
    "            means = []\n",
    "            for i in np.unique(labels):\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "                values = raw_clus[output].as_matrix()\n",
    "                weights = raw_clus[for_year('weight', year)].as_matrix()\n",
    "                average = np.average(values, weights=weights)\n",
    "                means.append(average)\n",
    "            sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "            print(sorted_ids)\n",
    "            mat['cluster'] = mat['cluster'].apply(lambda x: sorted_ids.index(x))\n",
    "#             vor = Voronoi(seg_data)\n",
    "#             plt.clf()\n",
    "#             voronoi_plot_2d(vor)\n",
    "#             plt.savefig(os.path.join(out_dir, 'voronoi.pdf'))\n",
    "            #mat['cluster_'+str(year)] = labels\n",
    "            select_variables = [output, 'latitude___ethiopia_2015', 'longitude___ethiopia_2015', 'nid'] + segment_vars\n",
    "            select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "\n",
    "#             id_df = pd.DataFrame(df.index.tolist())\n",
    "#             sel_df = pd.concat([, id_df], axis=0)\n",
    "            all_output = pd.concat([mat['cluster'], raw_df[list(select_variables)]], 1)\n",
    "            all_output.to_csv(os.path.join(out_dir,'clus_' + baseline + '_' + output + '.csv'))\n",
    "            return min_k\n",
    "            \n",
    "        \n",
    "        # Add segments\n",
    "        def add_segments():\n",
    "            median_segments = {}\n",
    "            for seg_var in segment_vars:\n",
    "                #binary\n",
    "                if len(np.unique(mat[seg_var])) <= 3:\n",
    "                    median_segments[seg_var] = 0\n",
    "                else:\n",
    "                    median_segments[seg_var] = np.median(mat[seg_var])\n",
    "            mat['cluster'] = 0\n",
    "            for seg_var in segment_vars:\n",
    "                mat['cluster'] = 2*mat['cluster'] + [int(x) for x in mat[seg_var] > median_segments[seg_var]]\n",
    "            #mat[['cluster'] + segment_vars].to_csv(os.path.join(out_dir,'segment_' + ','.join(segment_vars) + '_' + baseline + '_' + output + '.csv'))\n",
    "            return int(math.pow(2, len(segment_vars)))\n",
    "\n",
    "        def add_location_segments():\n",
    "            locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "            i = 0\n",
    "            mat['cluster'] = 0\n",
    "            for l in sorted(locations):\n",
    "                loc_feature = 'lives_in_' + l + '___ethiopia_' + str(year)\n",
    "                loc_val = mat[loc_feature].apply(lambda x: 0 if x < 0 else 1)\n",
    "                mat['cluster'] = mat['cluster'] + (i*loc_val)\n",
    "                i += 1\n",
    "            return len(locations) + 1\n",
    "            \n",
    "        def _run(max_clusters, method_name):\n",
    "            global table\n",
    "            global avg_table\n",
    "            global coef_map\n",
    "            # reg_clus keeps predictions from clustered regressions along with keys\n",
    "            reg_clus = dict()\n",
    "\n",
    "            for name, algo in regression_algorithms:\n",
    "                reg_clus[name] = {}\n",
    "\n",
    "            # need new dataframes with only training and test rows.\n",
    "            # we use this when looping through clusters\n",
    "            train_mat = mat.loc[ind_train]\n",
    "            test_mat = mat.loc[ind_test]\n",
    "            train_size = len(train_mat)\n",
    "            # same code as regular regressions, but add loop for clusters\n",
    "            series = {}\n",
    "            series[output] = []\n",
    "            for seg in segment_vars:\n",
    "                series[seg] = []\n",
    "            series = pd.DataFrame()\n",
    "            row = {}\n",
    "            raw_cols = raw_df.columns.values\n",
    "            raw_reg = re.compile('^((?!norm).)*2015$')\n",
    "            avg_variables = list(filter(raw_reg.search, raw_cols))\n",
    "#             for seg in segment_vars:\n",
    "#                 values = raw_df[seg].as_matrix()\n",
    "#                 weights = raw_df[for_year('weight', year)].as_matrix()\n",
    "#                 average = np.average(values, weights=weights)\n",
    "#                 row['weighted_mean_' + seg] = average\n",
    "#                 row['normal_mean_' + seg] = np.mean(values)\n",
    "#             new_row = pd.DataFrame(row, index=[0])\n",
    "#             avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "#             avg_table = avg_table.transpose()\n",
    "            for i in range(max_clusters):\n",
    "                train_clus = x_train.loc[train_mat['cluster'] == i]\n",
    "                train_y = y_train.loc[train_mat['cluster'] == i]\n",
    "                test_clus = x_test.loc[test_mat['cluster'] == i]\n",
    "                test_y = y_test.loc[test_mat['cluster'] == i]\n",
    "                sel_mat = mat[mat['cluster'] == i]\n",
    "                raw_clus = raw_df.loc[sel_mat.index]\n",
    "#                 avg_variables = segment_vars\n",
    "#                 avg_variables = [\n",
    "#                     'use_extension_program___policy',\n",
    "#                     'prevent_damage___policy',\n",
    "#                     'hired_labor___policy', \n",
    "#                     'oxen_owned___policy', \n",
    "#                     'chemical_fertilizers_used___policy', \n",
    "#                     'land_surface',\n",
    "#                     'plough_owned___policy',\n",
    "#                     'household_size',\n",
    "#                 ]\n",
    "#                 avg_variables = [for_year(x, year) for x in avg_variables]\n",
    "                for seg in avg_variables:\n",
    "                    values = raw_clus[seg].as_matrix()\n",
    "                    weights = raw_clus[for_year('weight', year)].as_matrix()\n",
    "                    average = np.average(values, weights=weights)\n",
    "                    row['mean_' + seg] = average\n",
    "                    row['avg_' + seg] = np.mean(values)\n",
    "                    variance = np.average((values-average)**2, weights=weights)\n",
    "                    row['stddev_' + seg] = math.sqrt(variance)\n",
    "                    row['stderr_' + seg] = math.sqrt(variance)/math.sqrt(len(values))\n",
    "                    row['25ile_' + seg] = np.percentile(values, 25)\n",
    "                    row['75ile_' + seg] = np.percentile(values, 75)\n",
    "#                     row['mean_' + seg] = np.mean(pd.concat([raw_clus[seg], test_clus[seg]], 0).as_matrix())\n",
    "#                 row['segment'] = ','.join(segment_vars)\n",
    "                row['index'] = i\n",
    "#                 row['mean_output'] = np.mean(raw_clus[output.replace('norm', 'raw')].as_matrix())\n",
    "#                 row['stddev_output'] = np.std(raw_clus[output.replace('norm', 'raw')].as_matrix())\n",
    "                row['size'] = len(raw_clus)\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "                avg_table = avg_table.append(new_row, ignore_index=True)\n",
    "                cluster_percent = (len(train_clus)*100.0)/train_size\n",
    "                if train_clus.empty or test_clus.empty: # or cluster_percent < 5:\n",
    "                    continue\n",
    "\n",
    "                keys_list = []\n",
    "                y_list = []\n",
    "                for k, v in test_y.iteritems():\n",
    "                    keys_list.append(k)\n",
    "                    y_list.append(v)\n",
    "\n",
    "                for name, algo in regression_algorithms:  \n",
    "#                     model = algo.fit(train_clus,train_y)\n",
    "#                     y_pred = model.predict(test_clus)\n",
    "                    model = sm.OLS(train_y, train_clus)\n",
    "                    fit = model.fit_regularized(alpha=1e-5, refit=True)\n",
    "#                     fit = model.fit()\n",
    "                    print(\"Confidence intervals are {0}\".format(fit.conf_int()))\n",
    "                    y_pred = fit.predict(test_clus)\n",
    "\n",
    "                    for a, b in enumerate(y_pred):\n",
    "                        t = reg_clus[name]\n",
    "                        t[keys_list[a]] = b\n",
    "\n",
    "                    coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = fit.params\n",
    "                    lower_bounds = []\n",
    "                    upper_bounds = []\n",
    "#                     ci = fit.conf_int()\n",
    "#                     for ci_row in ci.iterrows():\n",
    "#                         lower_bounds += [ci_row[1][0]]\n",
    "#                         upper_bounds += [ci_row[1][1]]\n",
    "                    for ci in fit.conf_int():\n",
    "                        lower_bounds += [ci[0]]\n",
    "                        upper_bounds += [ci[1]]\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_lower_bound'] = lower_bounds\n",
    "                    coef_map[name + '_' + method_name + '_' + str(i) + '_upper_bound'] = upper_bounds\n",
    "#                     coef_map[name + '_' + method_name + '_' + ','.join(segment_vars) + '_' + str(i)] = model.coef_\n",
    "\n",
    "            # plot sorted correlation\n",
    "            sorted_series = series.sort_values(['mean_' + output])\n",
    "#             prev = None\n",
    "#             min_diff = math.inf\n",
    "#             for mean_out in sorted_series['mean_output'].as_matrix():\n",
    "#                 if prev == None:\n",
    "#                     prev = mean_out\n",
    "#                     continue\n",
    "#                 diff = mean_out - prev\n",
    "#                 min_diff = min(min_diff, diff)\n",
    "            \n",
    "            \n",
    "            raw_segment_vars = avg_variables\n",
    "#             [t.replace('norm', 'raw') for t in segment_vars] + [\n",
    "#                 'gender_equity___ethiopia_2015', \n",
    "#                 'has_saved___policy___ethiopia_2015',\n",
    "#             ]\n",
    "            for seg in raw_segment_vars:\n",
    "                plt.clf()\n",
    "                plt.plot(sorted_series['mean_' + output].as_matrix(), sorted_series['mean_'+seg].as_matrix(), marker='o')\n",
    "                plt.xlabel('Average ' + output.replace('___output___ethiopia_2015', '') + ' output')\n",
    "                plt.ylabel('Average ' + seg.replace('___policy___ethiopia_' +str(year), '').replace('_',' '))\n",
    "                plt.savefig(os.path.join(out_dir, 'plot_' + seg + '_' + output + '.pdf'))\n",
    "                \n",
    "            # add mse's to table\n",
    "            keys = sorted(y_test.keys())\n",
    "            for name, algo in regression_algorithms:\n",
    "                sort_t = []\n",
    "                sort_p = []\n",
    "\n",
    "                for key in keys:\n",
    "                    if key not in y_test or key not in reg_clus[name]:\n",
    "                        continue\n",
    "                    sort_t.append(reg_clus[name][key])\n",
    "                    sort_p.append(y_test[key])\n",
    "\n",
    "                try:\n",
    "                    test_c = digitize(y, sort_t)\n",
    "                    pred_c = digitize(y, sort_p)\n",
    "                    auc_c = roc_auc_score(test_c, pred_c, average='macro')\n",
    "                except ValueError:\n",
    "                    auc_c = 0.5\n",
    "                mse = mean_squared_error(sort_t,sort_p)\n",
    "                scaled_mse = (mse/np.std(y))\n",
    "                new_row = pd.DataFrame({'model': name, 'segment': ','.join(segment_vars), 'input': baseline, 'scaled_mse': scaled_mse, 'mse': mse, 'roc_auc': auc_c, 'clustered': True, 'method': method_name}, index=[0])\n",
    "                table = table.append(new_row, ignore_index=True)\n",
    "                \n",
    "    \n",
    "        ##### Run grouped regressions\n",
    "        #_run(add_location_segments(), 'segmented')\n",
    "        if (len(segment_vars) > 1):\n",
    "            _run(add_clusters(), 'clustered')\n",
    "        #_run(add_segments(), 'segmented')\n",
    "    \n",
    "    def run_with_inputs(input_vars, name):\n",
    "        # map to be used in tracking coefficients\n",
    "        global coef_map\n",
    "        global coef_table\n",
    "        global x_train\n",
    "        global x_test\n",
    "        coef_map = {}\n",
    "        x_scaled, x_train, x_test = get_train_test(input_vars)\n",
    "#         update_best_lambda(x_scaled)\n",
    "        calc_unsegmented(name, x_train, x_test)\n",
    "#         univariate_segments = [[x] for x in segment_variables]\n",
    "#         from itertools import combinations\n",
    "        #for segment_vars in combinations(non_policy_inputs, 2):\n",
    "#         for segment_vars in univariate_segments:\n",
    "#             calc_segmented(segment_vars, name, x_train, x_test)\n",
    "#         for segment_vars in combinations(segment_variables, 2):\n",
    "#             calc_segmented(list(segment_vars), name, x_train, x_test)\n",
    "        calc_segmented(segment_variables, name, x_train, x_test)\n",
    "        # calc_segmented(['location'], name, x_train, x_test)\n",
    "#         q = table.loc[(table['input']==name)&\\\n",
    "#                       (table['clustered']==True)]['scaled_mse'].nsmallest(2)\n",
    "#         best_segments = list(table.iloc[q.index.values]['segment'])\n",
    "#         best_segments = best.loc[(best['input']==name)&\\\n",
    "#                                  (best['output']==output.split('___')[0])]['segment'].iloc[0].split(',')\n",
    "#         best_segments = [x + '___ethiopia_' + str(year) for x in best_segments]\n",
    "#         calc_segmented(best_segments, name, x_train, x_test)\n",
    "#         for segment_vars in best_segments:\n",
    "#             calc_segmented([segment_vars], name, x_train, x_test)\n",
    "        \n",
    "        for k,v in sorted(coef_map.items()):\n",
    "            kvp = dict()\n",
    "            kvp['model'] = k\n",
    "            kvp['inputs'] = name\n",
    "\n",
    "            for val,invar in zip(v,input_vars):\n",
    "                kvp[invar] = val\n",
    "\n",
    "            new_row = pd.DataFrame(kvp, index=[0])\n",
    "            coef_table = coef_table.append(new_row, ignore_index=True)\n",
    "    \n",
    "    #Baseline 1\n",
    "    input_vars = inputs + non_policy_inputs\n",
    "#     run_with_inputs(input_vars, 'All variables')\n",
    "#     #Baseline 2 - only highly correlated\n",
    "    imp_vars = re.compile('^.*(?=improved_seeds|extension|water_storage|saved|oxen|hired_workers|fertilizer|axe|credit).*$')\n",
    "    input_vars = list(filter(imp_vars.search, input_vars))\n",
    "    run_with_inputs(input_vars, 'Highly correlated policy and non-policy variables')\n",
    "#     #Baseline 3\n",
    "#     run_with_inputs(inputs, 'Policy variables')    \n",
    "\n",
    "    # save coefficient and output tables\n",
    "    coef_table.to_csv(os.path.join(out_dir,'coef_' + output + '.csv'))\n",
    "    table.to_csv(os.path.join(out_dir,output + '.csv'))\n",
    "    avg_table.to_csv(os.path.join(out_dir,output + '_avg' + '.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_segments(df, output):\n",
    "    df_t = df.loc[df[output].dropna().index]\n",
    "    df['segment_' + output], _ = get_classes(df_t[output], df[output])\n",
    "    return df\n",
    "\n",
    "def complete(df):\n",
    "    imp = Imputer(strategy=\"mean\")\n",
    "    completed = imp.fit_transform(df)\n",
    "\n",
    "    # if we want to use low rank matrix completion inst\n",
    "    # completed = SoftImpute().complete(x)\n",
    "\n",
    "    # reconstruct dataframe with completed matrix\n",
    "\n",
    "    cols = df.columns.values\n",
    "    mat = pd.DataFrame(completed,columns=cols)\n",
    "    mat_scaled = StandardScaler()\n",
    "    mat_scaled.fit(mat)\n",
    "    mat_sc = mat_scaled.transform(mat)\n",
    "    mat = pd.DataFrame(mat_sc, columns=mat.columns)\n",
    "    return mat\n",
    "\n",
    "def get_clusters(mat, output, segment_vars):\n",
    "    segment_vars = list(segment_vars.keys())\n",
    "    seg_data = mat[segment_vars]\n",
    "    min_k = 4\n",
    "    kmeans = KMeans(n_clusters=min_k).fit(seg_data)\n",
    "    labels = kmeans.labels_\n",
    "    #print(labels)\n",
    "    means = []\n",
    "    for i in np.unique(labels):\n",
    "        df_clus = mat.loc[labels == i]\n",
    "        means.append(np.mean(df_clus[output].as_matrix()))\n",
    "    sorted_ids = [i[0] for i in sorted(enumerate(means), key=lambda x:x[1])]\n",
    "    mat['segment_'+ output] = labels\n",
    "    mat['segment_'+ output] = mat['segment_'+ output].apply(lambda x: sorted_ids.index(x))\n",
    "#     raw_df['nid'] = df['nid']= df.index.tolist()\n",
    "    select_variables = [output, 'latitude___ethiopia_2015', 'longitude___ethiopia_2015'] + segment_vars\n",
    "    select_variables = [t.replace('norm', 'raw') for t in select_variables]\n",
    "#             print (mat)\n",
    "\n",
    "#             id_df = pd.DataFrame(df.index.tolist())\n",
    "#             sel_df = pd.concat([, id_df], axis=0)\n",
    "    all_output = pd.concat([mat['segment_' + output], raw_df[select_variables]], 1)\n",
    "    all_output.to_csv(os.path.join('outputs_v22_both7_crop_sales','clus_' + '_' + output + '.csv'))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number_of_hired_workers___policy___ethiopia_2015': 0.273808267736554, 'land_surface___ethiopia_2015': 0.272516052599146, 'quantity_of_chemical_fertilizers_used___policy___ethiopia_2015': 0.160090553373268, 'number_of_oxen_owned___policy___ethiopia_2015': 0.157053494140115, 'household_size___ethiopia_2015': 0.14258227392332698, 'number_of_plough_owned___policy___ethiopia_2015': 0.14223988258113301, 'uses_extension_program___policy___ethiopia_2015': 0.12981246190952803}\n",
      "['crop_sales___output___ethiopia_2015']\n",
      "Dir exists\n",
      "Confidence intervals are [[ 0.02974554  0.12515473]\n",
      " [-0.00422201  0.09267721]\n",
      " [ 0.21031701  0.30627738]\n",
      " [ 0.03066054  0.13384061]\n",
      " [-0.10902334 -0.01164783]\n",
      " [ 0.00423478  0.09765939]\n",
      " [ 0.01555246  0.15004023]\n",
      " [-0.05366113  0.04766206]\n",
      " [-0.07706795  0.0195646 ]\n",
      " [-0.05180386  0.0722242 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ananth/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:271: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 3, 1]\n",
      "Confidence intervals are [[-0.048559    0.14519449]\n",
      " [ 0.0844788   0.30772241]\n",
      " [ 0.49562947  0.89355246]\n",
      " [-0.12789796  0.13543187]\n",
      " [-0.25771298 -0.03002175]\n",
      " [ 0.03795755  0.31423419]\n",
      " [ 0.17749701  0.53471203]\n",
      " [-0.02696488  0.33544742]\n",
      " [-0.16710823  0.07151559]\n",
      " [-0.28006383  0.0121802 ]]\n",
      "Confidence intervals are [[-0.13899989  0.07065103]\n",
      " [-0.11906094  0.06670665]\n",
      " [ 0.06220702  0.57027408]\n",
      " [-0.14751715  0.06584407]\n",
      " [-0.14027674  0.05478176]\n",
      " [-0.07186288  0.13343291]\n",
      " [-0.30016596  0.01709062]\n",
      " [-0.23467942  0.15436536]\n",
      " [-0.12138359  0.12617301]\n",
      " [-0.08945013  0.22098663]]\n",
      "Confidence intervals are [[ 0.06812117  0.22909869]\n",
      " [-0.04368405  0.11362392]\n",
      " [-0.17981068  0.28150232]\n",
      " [ 0.04431765  0.21779457]\n",
      " [-0.11954748  0.03020123]\n",
      " [-0.0661047   0.07031422]\n",
      " [-0.08848345  0.13289227]\n",
      " [-0.06988322  0.05297529]\n",
      " [-0.09054811  0.04101795]\n",
      " [-0.07016739  0.11138549]]\n",
      "Confidence intervals are [[-0.00810656  0.18515091]\n",
      " [-0.04799348  0.18710486]\n",
      " [ 0.20733308  0.33158621]\n",
      " [-0.1585369   0.06450058]\n",
      " [-0.22226084  0.01896506]\n",
      " [ 0.02069439  0.20878424]\n",
      " [-0.05079325  0.27072757]\n",
      " [-0.0960604   0.11343485]\n",
      " [-0.15334837  0.06304528]\n",
      " [-0.23158375  0.08400899]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "filename = '/home/ananth/Downloads/ethiopia_v23_normed.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# change to true if you want to use all input fields\n",
    "def get_vars(year, base_output):\n",
    "    base_suffix = '___ethiopia_{0}'.format(year)\n",
    "    year_vars = df.filter(regex='.*{0}'.format(base_suffix)).columns.values\n",
    "    non_raw_reg = re.compile('^((?!gender|damaged|bank|price|irrigation|diversification).)*$')\n",
    "    year_vars = list(filter(non_raw_reg.search, year_vars))\n",
    "    out_reg = re.compile('.*' + base_output + '___output.*$')\n",
    "    outputs = list(filter(out_reg.search, year_vars))\n",
    "    policy_reg = re.compile('(.*___policy.*)$')\n",
    "    policy_inputs = list(filter(policy_reg.search, year_vars))\n",
    "    non_policy_reg = re.compile('^((?!policy|output|weight).)*$')\n",
    "    non_policy_inputs = list(filter(non_policy_reg.search, year_vars))\n",
    "    \n",
    "    return outputs, policy_inputs, non_policy_inputs\n",
    "\n",
    "# years = [2011, 2013, 2015]\n",
    "years = [2015]\n",
    "base_segment_variables = [\n",
    "    'use_extension_program___policy',\n",
    "    'prevent_damage___policy',\n",
    "    'hired_labor___policy___norm', \n",
    "    'oxen_owned___policy___norm', \n",
    "    'chemical_fertilizers_used___policy___norm',\n",
    "    'gender_equity',\n",
    "    'has_saved___policy',\n",
    "#     'land_surface',\n",
    "#     'plough_owned___policy',\n",
    "#     'household_size',\n",
    "]\n",
    "\n",
    "for base_output in ['crop_sales',]:\n",
    "#                     'crop_sales_growth',\n",
    "#                     'expenditure',\n",
    "#                     'food_expenditure_diversification',\n",
    "#                     'has_medical_assistance',\n",
    "#                     'no_food_deficiency',\n",
    "# #                     'productivity',\n",
    "# #                     'productivity_growth',\n",
    "#                     'children_education']:\n",
    "    ccs = pd.read_csv('/home/ananth/Downloads/cross_correls_ethiopia_2015_v22.csv')\n",
    "    output = base_output + '___output___ethiopia_2015' #'average___ethiopia_2015' #'crop_sales___output___ethiopia_2015'##\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    # policy = ccs['Unnamed: 0'].apply(lambda x: x.find(\"___policy\") != -1)\n",
    "    # no_lives = ccs['Unnamed: 0'].apply(lambda x: x.find(\"lives_in\") == -1)\n",
    "    # no_lat = ccs['Unnamed: 0'].apply(lambda x: x.find(\"latitude\") == -1)\n",
    "    # no_dist = ccs['Unnamed: 0'].apply(lambda x: x.find(\"distance\") == -1)\n",
    "    # no_lon = ccs['Unnamed: 0'].apply(lambda x: x.find(\"longitude\") == -1)\n",
    "    # no_equ = ccs['Unnamed: 0'].apply(lambda x: x.find(\"equal\") == -1)\n",
    "    # ccs = ccs[policy & no_lives & no_lat & no_lon & no_equ & no_dist]\n",
    "    \n",
    "#     ### Choose from selected variables\n",
    "    select = ccs['Unnamed: 0'].str.contains('^.*(oxen|extension|hired|chemical|plough|land_surface|household_size)') # gender, has_saved\n",
    "    ccs = ccs[select]\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    num_vars=8\n",
    "    \n",
    "    best_vars = ccs[['Unnamed: 0', output]][:num_vars].as_matrix()\n",
    "    segment_variables = {}\n",
    "    seg_vars = []\n",
    "    for i in best_vars:\n",
    "#         if i[0].find('hired') != -1 or i[0].find('chemical') != -1 or i[0].find('oxen')!= -1:\n",
    "#             name = i[0].replace('policy', 'policy___norm')\n",
    "#         elif i[0].find('gender') != -1:\n",
    "#             name = i[0].replace('___policy', '')\n",
    "#         elif i[0].find('price')!= -1:\n",
    "#             name = i[0].replace('policy', 'policy___raw')\n",
    "#         elif i[0].find('land_surface')!= -1:\n",
    "#             name = i[0].replace('ethiopia', 'norm___ethiopia')\n",
    "#         else:\n",
    "#             name = i[0]\n",
    "#         seg_vars.append(name)\n",
    "        name = i[0]\n",
    "        if 'lives' in name or 'distance' in name:\n",
    "            continue\n",
    "        segment_variables[name] = abs(i[1])\n",
    "\n",
    "    print(segment_variables)\n",
    "\n",
    "    raw_df = df.copy()\n",
    "    df = complete(df)\n",
    "    for year in years:\n",
    "        base_suffix = '___ethiopia_{0}'.format(year)\n",
    "    #     segment_variables = [x+base_suffix for x in base_segment_variables]\n",
    "        year_segment_variables = {}\n",
    "        for k, v in segment_variables.items():\n",
    "            year_segment_variables[k.replace('2015', str(year))] = v\n",
    "        outputs, policy_inputs, non_policy_inputs = get_vars(year, base_output)\n",
    "        print(outputs)\n",
    "        for output in outputs:\n",
    "#             df = get_clusters(df, output, year_segment_variables)\n",
    "            run_regressions(in_name=filename, out_dir='./outputs_v23_prod7_' + base_output, non_policy_inputs=non_policy_inputs, segment_variables=segment_variables, inputs=policy_inputs, output=output, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       7278.607562\n",
       "1       7162.340724\n",
       "2               NaN\n",
       "3               NaN\n",
       "4               NaN\n",
       "5               NaN\n",
       "6               NaN\n",
       "7       7161.557051\n",
       "8       7166.854291\n",
       "9       6857.252532\n",
       "10      6290.982128\n",
       "11              NaN\n",
       "12      5782.071500\n",
       "13      6523.213127\n",
       "14              NaN\n",
       "15      7033.262202\n",
       "16      6553.770991\n",
       "17      4476.888948\n",
       "18      5233.366260\n",
       "19      6450.569270\n",
       "20      5576.814515\n",
       "21      7359.835183\n",
       "22              NaN\n",
       "23              NaN\n",
       "24              NaN\n",
       "25              NaN\n",
       "26              NaN\n",
       "27              NaN\n",
       "28              NaN\n",
       "29              NaN\n",
       "           ...     \n",
       "3609            NaN\n",
       "3610            NaN\n",
       "3611            NaN\n",
       "3612    4883.687465\n",
       "3613            NaN\n",
       "3614    5653.899319\n",
       "3615            NaN\n",
       "3616            NaN\n",
       "3617    2929.401621\n",
       "3618    7818.236758\n",
       "3619            NaN\n",
       "3620    7520.345142\n",
       "3621            NaN\n",
       "3622            NaN\n",
       "3623    8138.641828\n",
       "3624    4764.808693\n",
       "3625    6837.800596\n",
       "3626    9795.836170\n",
       "3627            NaN\n",
       "3628            NaN\n",
       "3629            NaN\n",
       "3630            NaN\n",
       "3631            NaN\n",
       "3632            NaN\n",
       "3633            NaN\n",
       "3634            NaN\n",
       "3635            NaN\n",
       "3636            NaN\n",
       "3637            NaN\n",
       "3638            NaN\n",
       "Name: test, Length: 3639, dtype: float64"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['test'] = raw_df['crop_sales___output___ethiopia_2015']/raw_df['land_surface___ethiopia_2015']*10000.0\n",
    "df['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['number_of_hired_workers___policy___ethiopia_2015',\n",
       "        0.2738082677365537],\n",
       "       ['land_surface___ethiopia_2015', 0.27251605259914585],\n",
       "       ['average_precipitation___ethiopia_2015', 0.2294706219248923],\n",
       "       ['quantity_of_chemical_fertilizers_used___policy___ethiopia_2015',\n",
       "        0.16009055337326802],\n",
       "       ['number_of_oxen_owned___policy___ethiopia_2015',\n",
       "        0.15705349414011496],\n",
       "       ['prevent_damage___policy___ethiopia_2015', 0.15313691644432134],\n",
       "       ['gender_equity___policy___ethiopia_2015', 0.15266280975569105],\n",
       "       ['elevation___ethiopia_2015', 0.1522841010209675],\n",
       "       ['household_size___ethiopia_2015', 0.14258227392332665],\n",
       "       ['number_of_plough_owned___policy___ethiopia_2015',\n",
       "        0.14223988258113285],\n",
       "       ['uses_extension_program___policy___ethiopia_2015',\n",
       "        0.12981246190952828],\n",
       "       ['household_head_is_male___ethiopia_2015', 0.12555273470354533],\n",
       "       ['average_temperature___ethiopia_2015', 0.12206105019205225],\n",
       "       ['uses_irrigation___policy___ethiopia_2015', 0.1030213152351026],\n",
       "       ['has_saved___policy___ethiopia_2015', 0.102381473336001]], dtype=object)"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccs = pd.read_csv('cross_correls_ethiopia_2015_v22.csv')\n",
    "policy = ccs['Unnamed: 0'].apply(lambda x: x.find(\"___policy\") != -1)\n",
    "no_lives = ccs['Unnamed: 0'].apply(lambda x: x.find(\"lives_in\") == -1)\n",
    "no_lat = ccs['Unnamed: 0'].apply(lambda x: x.find(\"latitude\") == -1)\n",
    "no_dist = ccs['Unnamed: 0'].apply(lambda x: x.find(\"distance\") == -1)\n",
    "no_lon = ccs['Unnamed: 0'].apply(lambda x: x.find(\"longitude\") == -1)\n",
    "no_equ = ccs['Unnamed: 0'].apply(lambda x: x.find(\"equal\") == -1)\n",
    "ccs = ccs[no_lives & no_lat & no_lon & no_equ & no_dist]\n",
    "cols = ccs.columns.values\n",
    "best_var_series = {}\n",
    "for output in cols:\n",
    "    #output = 'average___ethiopia_2015'#'crop_sales___output___ethiopia_2015'#\n",
    "    if output == 'Unnamed: 0':\n",
    "        continue\n",
    "    ccs[output] = ccs[output].apply(lambda x: abs(x))\n",
    "    ccs = ccs.sort_values(output, ascending=False)\n",
    "    best_vars = ccs[['Unnamed: 0', output]][:15].as_matrix()\n",
    "#     best_vars = [i[0] for i in best_vars]\n",
    "    best_var_series[output] = best_vars\n",
    "best_var_series['crop_sales___output___ethiopia_2015']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnX2QXNV14H9nWi3owQ4zMrIXBskC\nlhVBJkhGBfKqymVwjDAYM/4gmIBDsq6QytqJYV2KJS8bgYODskps7C3HW9hmgxcWhIGMZZAjWIMr\ntawlkBjJsgwK4sOSWlozDhoMTIN6Zs7+0e8Nr/t9d7+e/jq/qpFm7rvvvvte97vn3nPOPUdUFcMw\nDMPw0tfqDhiGYRjthwkHwzAMw4cJB8MwDMOHCQfDMAzDhwkHwzAMw4cJB8MwDMOHCQfDMAzDhwkH\nwzAMw4cJB8MwDMPHnFZ3oF5OOOEEXbRoUau7YRiG0VHs2LHj16o6P65exwqHRYsWsX379lZ3wzAM\no6MQkV8mqWdqJcMwDMOHCQfDMAzDhwkHwzAMw4cJB8MwDMOHCQfDMAzDR8d6KxlGtzIyWmTDlr0c\nGi9x0kCB1asWM7xsKLAcSFx3eNlQi++sfpp9P2naj6rbTc9dOjUT3PLly9VcWY1uY2S0yNoHdlMq\nT82UFfI5PnHOEPfvKFaV53MCCuVpja1byOe45eNndeRAFfZMsrqfNO1H1QWa2s+sEJEdqro8rp6p\nlQyjjdiwZW/V4AJQKk9x97YDvvLylFYJhqi6pfIUG7bsbU6nm0zYM8nqftK0H1W32f2cbUytZBht\nxKHxUmD5VIoVfljdsLbbnbB+Z3U/adqvpy+d+txt5WAYbcRJA4XA8pxI4jbC6oa13e6E9Tur+0nT\nflTdZvdztjHhYBhtxOpViynkc1VlhXyOK89b4CvP54R8nySqW8jnZgzYnUbYM8nqftK0H1W32f2c\nbUytZBhthGu4DPJ4Wf7ueYm9lYLqtpNRNA1Rz2S2209St1ueu3krGYZh9BDmrWQYhmHUjQkHwzAM\nw4cJB8MwDMOHCQfDMAzDhwkHwzAMw4cJB8MwDMOHCQfDMAzDhwkHwzAMw0escBCRY0XkCRHZJSJ7\nROQmp/wUEdkmIs+KyEYRmeuUH+P8vc85vsjT1lqnfK+IrPKUX+SU7RORNdnfpmEYhpGGJCuHN4EL\nVPVsYClwkYisAP4G+Jqqng4cAT7j1P8McERV/y3wNaceInIm8ClgCXAR8PcikhORHPBN4MPAmcCV\nTl3DMAyjRcQKB63wmvNn3vlR4ALgPqf8DmDY+f0y52+c4x8UEXHK71HVN1X1BWAfcK7zs09Vn1fV\no8A9Tl3DMAyjRSSyOTgz/J3AS8AjwHPAuKpOOlUOAm50qSHgAIBz/BXgHd7ymnPCyg3DMIwWkUg4\nqOqUqi4FTqYy0//toGrO/0HB5LWOch8icq2IbBeR7WNjY/EdNwzDMOoilbeSqo4DPwFWAAMi4ob8\nPhk45Px+EFgA4Bw/HnjZW15zTlh50PVvU9Xlqrp8/vz5abpuGIZhpCCJt9J8ERlwfi8Avws8DTwG\nfNKpdg3wA+f3Tc7fOMcf1Upc8E3ApxxvplOA04EngCeB0x3vp7lUjNabsrg5wzAMoz6SJPs5EbjD\n8SrqA+5V1QdF5BfAPSJyMzAKfNep/13gf4rIPiorhk8BqOoeEbkX+AUwCXxWVacARORzwBYgB9yu\nqnsyu0PDMAwjNZbsxzAMo4ewZD+GYRhG3ZhwMAzDMHyYcDAMwzB8mHAwDMMwfJhwMAzDMHyYcDAM\nwzB8mHAwDMMwfJhwMAzDMHyYcDAMwzB8mHAwDMMwfJhwMAzDMHyYcDAMwzB8mHAwDMMwfJhwMAzD\nMHyYcDAMwzB8mHAwDMMwfJhwMAzDMHyYcDAMwzB8mHAwDMMwfJhwMAzDMHyYcDAMwzB8xAoHEVkg\nIo+JyNMiskdEPu+U3ygiRRHZ6fxc7DlnrYjsE5G9IrLKU36RU7ZPRNZ4yk8RkW0i8qyIbBSRuVnf\nqGEYhpGcJCuHSeALqvrbwArgsyJypnPsa6q61PnZDOAc+xSwBLgI+HsRyYlIDvgm8GHgTOBKTzt/\n47R1OnAE+ExG92cYhmHUQaxwUNXDqvqU8/urwNPAUMQplwH3qOqbqvoCsA841/nZp6rPq+pR4B7g\nMhER4ALgPuf8O4Dhem/IMAzDaJxUNgcRWQQsA7Y5RZ8TkZ+JyO0iMuiUDQEHPKcddMrCyt8BjKvq\nZE25YbQVI6NFVq5/lFPWPMTK9Y8yMlpsdZcMo2kkFg4i8jbgfuA6Vf0N8C3gNGApcBj4O7dqwOla\nR3lQH64Vke0isn1sbCxp1w2jYUZGi6x9YDfF8RIKFMdLrH1gtwkIo2tJJBxEJE9FMNylqg8AqOqv\nVHVKVaeBb1NRG0Fl5r/Ac/rJwKGI8l8DAyIyp6bch6repqrLVXX5/Pnzk3TdMDJhw5a9lMpTVWWl\n8hQbtuxtUY8Mo7kk8VYS4LvA06r6VU/5iZ5qHwN+7vy+CfiUiBwjIqcApwNPAE8CpzueSXOpGK03\nqaoCjwGfdM6/BvhBY7dlGNlyaLyUqtwwOp058VVYCXwa2C0iO52yL1HxNlpKRQX0IvAnAKq6R0Tu\nBX5BxdPps6o6BSAinwO2ADngdlXd47T3ReAeEbkZGKUijAyjbThpoEAxQBCcNFBoQW8Mo/lIZeLe\neSxfvly3b9/e6m4YPYJrc/Cqlgr5HLd8/CyGl5n/hNE5iMgOVV0eVy/JysEweh5XAGzYspdD4yVO\nGiiwetViEwxG12LCwTASMrxsyISB0TNYbCXDMAzDhwkHwzAMw4cJB8MwDMOHCQfDMAzDhwkHwzAM\nw4cJB8MwDMOHCQfDMAzDhwkHwzAMw4dtgjPakpHRYuhu5Khj3czIaJGbfriHIxNlAAYKeW786BLf\nvdfzfNKe06ufQS9hwsFoO2rjGLm5E1zCjnXz4DQyWmT1fbsoT70VC228VGb193cBVAnOtM8n7Tn1\nXMPoPEytZLQdUbkTejWvwoYte6sEg0t5WqvuvZ7nk/acXv0Meg1bORhtRz25E7o9r0LSe8/y2WVV\nbnQmtnIw2o6wHAknDRQij3UzUffnPVbP80l7Tq9+Br2GCQej7Vi9ajGFfK6qrJDPsXrV4shj3czq\nVYvJ5/zp1vN9UnXv9TyftOf06mfQa5hayWg7kuRO6CZPmaSeWccX8pSnpnn9aEXfH+StlCbvRG3b\nx+b7GJ8oxz5T7zWK4yVyIlU2hzSeUQP9eVThlVL8deOelZEtlgnOMFpIVIY5oGnZ57LIbFdPG0Hn\neIk637LxZUPSTHCmVjKMFtIqz6ws2s7KMyrp+eYlNbuYWskwWkirPLOy8DhqVt/NS6o9sJWDYbSQ\nVnlmZdF2lp5RjbRrXlLNwYSDYbSQVnlmZdF2Vp5RSc83L6nZJVY4iMgCEXlMRJ4WkT0i8nmnfJ6I\nPCIizzr/DzrlIiLfEJF9IvIzEXmvp61rnPrPisg1nvJzRGS3c843RMTvs2cYXcjwsiFu+fhZDA0U\nEGBooDBjYI061szrNrON2nMG+/MMFPKJzm/m8zD8xHoriciJwImq+pSIvB3YAQwDfwi8rKrrRWQN\nMKiqXxSRi4E/Ay4GzgO+rqrnicg8YDuwHFCnnXNU9YiIPAF8HtgKbAa+oao/iuqXeSsZhmGkJzNv\nJVU9rKpPOb+/CjwNDAGXAXc41e6gIjBwyr+nFbYCA46AWQU8oqovq+oR4BHgIufYb6nqT7Uiqb7n\nacswDMNoAalsDiKyCFgGbAPepaqHoSJAgHc61YaAA57TDjplUeUHA8qDrn+tiGwXke1jY2Npum4Y\nhmGkILFwEJG3AfcD16nqb6KqBpRpHeX+QtXbVHW5qi6fP39+XJcNwzCMOkkkHEQkT0Uw3KWqDzjF\nv3JUQq5d4iWn/CCwwHP6ycChmPKTA8oNwzCMFpHEW0mA7wJPq+pXPYc2Aa7H0TXADzzlf+B4La0A\nXnHUTluAC0Vk0PFsuhDY4hx7VURWONf6A09bhmEYRgtIskN6JfBpYLeI7HTKvgSsB+4Vkc8A+4HL\nnWObqXgq7QMmgD8CUNWXReSvgCedel9W1Zed3/8U+AegAPzI+TEMwzBahAXeMwzD6CEs8J5hGIZR\nNyYcDMMwDB8mHAzDMAwfJhwMwzAMHyYcDMMwDB8mHAzDMAwflgnOMJrAyGiRDVv2cmi8xEkDBVav\nWtyU0NJZXMfbxvGFPCIwPlFO3F5cH0ZGi9y4aQ/jpTJQCdO97tIlM3Vm61k1Sqf0Mytsn4NhpOSG\nkd3cve0AU6rkRLjyvAXcPHzWzPGR0SJrH9hdle+4kM9lnntgZLTI6u/vojz91juc7xM2XH524usE\n9bWWq1csZPm75wUOjHH3GtRHl+Pm5vjYe4fY+MQB3z1cce4CHntmLDOh1+hgPluf6WyQdJ+DCQfD\nSMENI7u5c+t+X/nVKxbOCIiV6x+lGJDXeGigwONrLsisL0tvenhmNu5loJBn57oLE7UR1tda8jmh\nPPXWWOEOjBu27I2816Ttx5F2IM56MJ+tz3Q2sE1whtEE7t52ILY8LOF9WHm9BAmGqPIgkvbJKxgA\nSuWpmVl5VLtZ3bN7vaRs2LLXtxpK24aX2fpM2wkTDoaRgqmQlba3PCzhfVh5K2mkT666JqrdLO85\nzUCc9WDeSZ9pVphwMIwU5ELSm3vLV69aTCGfqzpeyOdYvWpxpn0Z7M+nKg8iqK9JcfX4Ufe6etVi\n8n3ZpIRPMxBnPZjP1mfaTphwMIwUXHnegtjy4WVD3PLxsxgaKCBU9NLNMFyuu3QJ+Vz1wJvPCesu\nXZK4DW9fw+hz2vXiDoxx9zq8bIgNl59NIR881OT6hFrZ0Sfh10tK1oP5bH2m7YQZpA0jJXHeSrNJ\n1u6VI6NFbvrhHo5MVOwWA4U8N360ImyycJkNcmkNajur6/WS62lSzFvJMAzD8GHeSoZhGEbd2A5p\no+MwdUH7kGR3dNjmuahd2cDMHoqcCFOqDPbnebM8xUR5GgABlIr+374D2WNqJaOj6Kadqp1Okt3R\nQcc/cc4Q9+8ohu7KzucElMBd1WHYdyA5plYyupKsNzcZ9RP3WYQdv3vbgchwHeUpTSUYaq9rZIMJ\nB6Oj6MWdqu1KvbujwzYSNqs/Rn2YcDA6il7cqdqu1Ls7OmwjYbP6Y9RHrEFaRG4HPgK8pKrvccpu\nBP4YGHOqfUlVNzvH1gKfAaaAP1fVLU75RcDXgRzwHVVd75SfAtwDzAOeAj6tqkezukGju1i9anGg\nHjvrnapJDKkDNQZSd09Abb16jOa15y96R4Gtzx9hShUB+ufmmDg65Wu79rzzz5jfcHTTMOI+i7Dj\nzbI5dPNu5VYQa5AWkfcDrwHfqxEOr6nq39bUPRO4GzgXOAn438C/cw7/C/Ah4CDwJHClqv5CRO4F\nHlDVe0TkvwO7VPVbcR03g3TvknQTWr0DdL2GVHgr3HRtvTQG0yRhtL24XjsDhTyvH530Bcnz0qjh\nNq3wMW+l9iPTTXAisgh4MIFwWAugqrc4f28BbnQO36iqq7z1gPVUVh//RlUnReR93npRmHDoTZJ6\nKzXi1RQWntkdpOIIq5c0vHNWYa7DqDfMtHmKdQez4a30ORH5mYjcLiKDTtkQ4I1pfNApCyt/BzCu\nqpM15YGIyLUisl1Eto+NjYVVM7qYpN5KjXg1NWpIDauX1GDabMNqve2bp1hvUa9w+BZwGrAUOAz8\nnVMeZGnSOsoDUdXbVHW5qi6fP39+uh4bXUFSb6VGvJoaNaSG1UtqMG22YbXe9s1TrLeoa4e0qv7K\n/V1Evg086Px5EPCGrTwZOOT8HlT+a2BAROY4qwdvfcPwcdJAIVDlcmy+j9PWbp6xQxyb76Pk6KZr\nz4+jXkOqy4pTB3lq/ytV9QQojpdm+lirJ3d18MXxUuCMKUte+k2JRWseAqA/38cx+RxHJsoz6rA+\nAdcW7A281xeiLot6pmntPmH2JO/zccmJsOLUQV7811LT8l/3MnUJBxE5UVUPO39+DPi58/sm4H+J\nyFepGKRPB56g8m6c7ngmFYFPAb+vqioijwGfpOKxdA3wg3pvxuh+ggbuPqgSBFOqlMpKH+AVD0k9\nWtzBIWjQ8OZSHujP8/qbkxytMQA/tf8VPnHOEI89MzYz2Ls13MG1OF5i7QO7Z87x3lMjuwDyfcLb\njp3DkYly1XW9eGXmRHl6xsDr9s3rJDReKvOfNu4klwsWDFHPtNZG4b3noAG4NgXrlCp3bt3PC2Ov\n+YSte/zx516u6qtL3LXq6V+vkcRb6W7gA8AJwK+Adc7fS6l8914E/sQVFiLyn4H/AEwC16nqj5zy\ni4Fbqbiy3q6qX3HKT+UtV9ZR4GpVfTOu42aQ7l1qZ3uHXykR5PXYJ3Di8YWmzgrjcgvHGZfdPAqN\nGKBzIkyrBrq1fuHeXU3bdJYT4e9+7+zQZ5o277K7qsqSKON7N+WFTkNSg3TsykFVrwwo/m5E/a8A\nXwko3wxsDih/norrq2EkYnjZUNWA5KpIaplWmv6SN5pDOQt9/bQqL6y/xFc+vGyI6zfubLj9qOtG\nCdu0NopmCLGo52s2lGhsh7TR8SRJ3dksGs2hfNJAoWEDdNT5zTRuJ7m3NOXN+LzqeTa207qCCQej\n40mSurNZJMmhHJaj2a0XVSffJ+QicjDH2VEayRHtEpUmNIq0qTrDPq+Vp82r6x7qeTa20/otLJ+D\n0fG4u6Nbkbozynhde9y72zdoV29YHaAqdWeancG1/Tu+kOfo5Fu7jNN4K6X16ol7NrVEfY7N8FZK\n279ew/I5GEbGJAkZMZsxkcJyN9fTXlzokiqXXAF3eHF/r909HibIzcW0eVgO6QDsC2c0mzRxmdzQ\nE0BsLKV6w1SMjBZZ/f1dviB2+Zyw4ZPhnkZB1Lqauly9YuHM7D5NTKigNtw+B7UTFthwoD+PKrxS\nqo7N5BWItec3SiePJSYcarC4MMZskDYuUxpX1npcLKNcaePaS+oynBPhuVsubjgmlKsm86rQaink\nc7x34fFV+xvSkO8TNlyeTijW0uljiWWCq8HiwhizQVo3zUPjpabGXKrHlRPeGgCL4yWUivAKi6Dt\n3djXCMXxEqvv2xUqGKDyztYrGKASBrzRd75XxpKeMUibT3Nvk0bfH6ciiKofFt4jbOVwUoqVQ5CL\nZVzfw/oDcHwh72vHaxBPwykhe03SEhVuPCsafed7ZSzpGeEQ9pKYT3P7k0XinLAwCUCqEApxIRfS\nxGXyuk0msTmcf8Z8Vq5/tMpo7W0zqO+rVy0OtDkAvPrmJEtvepjxUjkwxEcaOkk5ncW+kl4YS8zm\n0CF6wl4li88tKkwCBM/aa/XxQa6UYfW9Hj3e414PJNft8shEucp1NIg+gfedOi8wvlAYtcbb6+/d\nSYe+6pliNgezOfgYXjbELR8/i6GBAkLlZe2UD7OXyUK/G6UGSKIi8Orf464xMlrk/h1F3+y7OF7i\n/h1FVq9azNeuWMqbk9MzuvW4bJjTCv/3uZdTeQGNl8qs/v4uRkaLle94hwiGfE7IR2z6a4SBQr5h\nwQC9M5b0jFoJ/DF5jPYnC/1unBogTkUQJKDC6kfV9Qq1tO6e9YztrvF1eNlQpO0hCUMNnp+EnAjl\nqUo6UK9r6pHX35zZtFdPm7XBAevZh1JLL4wlPbNyMDqTLOLfRIVJWL1qsS80RD4nVSEU4gSR13aQ\nJNDebBou3WvVG0ajT+DWK5by+JoLmh6ryl1tHZkoM14qc3whz/lnzA+0lwDk+uJXGVOqrH1gNyOj\nRSDYC2vtA7u5YWR3YLl7Xi9iwsFoa7KIfxOrBqgde2r+jhJEtW3FCa2B/jx9dQ6y9Zzl9sd9BoP9\n+ZgzqpnWSuiOU9Y8xNw5zQ9k6GW8VObOrftDPZimppW3HTtn5nMd7K/YcWrxrtjC1JR3bzvQE+6p\naegptZLReWQV/yZMDbBhy17fzNSrjoHwzHBBeuagui75nPDaG5N1eQO5Hk9uAqEk5PvEJ0TfqFHP\nhCUE8uLaRkrlaV8CJS8DhXzVjuQo6nGXDWJ8oszoX14483eYS21cCPVG8353IyYcjLanmfrdJDaN\nNAIqKtDe629Oxg6e7mA9EBNErjbG0YpTB/nF4VdnBvKgUBFBs2Y3gB8k22sRpfl/pVROZJsYGihk\nNujWrtTi7Ev17kPpRUw4GD1NUp/1NAIqrG7YrFYgMFlPFDcPn5U66myUIPzaFUu5fuPOhpyaXCEW\ntWfDVQlGuQUnRcC3Mgpb5XlDqNezD6UXMZuD0dPMZkz/VieXibr+8LIhrlqxMJFdY7A/H/rMau07\nA4V8xRZAtX1m9arFDbusXrVioU8Ix9mXwo7fPHxWT7inpqFnNsEZRhizFWGz1Zunklzf+yyOL+R5\n/ehklUHYG0m20WdWG0q80n4fApGuq0JFMMxGvo5uxKKyGkYb0upQz1nGkZqtfqZN4mNEY8LB6Fpa\nPcAGEZQEZ/m756VK4BOU8Of+HQcpObPoPoHfP88/Y457HlHJfmqPHTc3h6rOzNwL+T6mFd6crPxd\nO2tvx8/CiCYz4SAitwMfAV5S1fc4ZfOAjcAi4EXg91T1iIgI8HXgYmAC+ENVfco55xrgBqfZm1X1\nDqf8HOAfgAKwGfi8JpBYJhx6k1arZoIIS4ITFzPJ2+80iXKSJMaBt9JoPvHCkcCNZMfNzfH60fSJ\neVwG+/O89sZkVdtel1sTGO1JlsLh/cBrwPc8wuG/Ai+r6noRWQMMquoXReRi4M+oCIfzgK+r6nmO\nMNkOLKfiPbcDOMcRKE8Anwe2UhEO31DVH8V13IRDbxIVRC9N4posB6zT1m6u22ff7XeaRDluch2I\nTubTbtTmvLZVR2tIKhxiXVlV9Z9FZFFN8WXAB5zf7wB+AnzRKf+eM/PfKiIDInKiU/cRVX3Z6dwj\nwEUi8hPgt1T1p07594BhIFY4GL1JPbGW4sJsN0ojm7niNmfFXa+TNmk1EirdmH3qdWV9l6oeBnD+\nf6dTPgQc8NQ76JRFlR8MKDeMQOpxB2125q5GYg55N2fVc71O26TlPvesP5OR0SIr1z/KKWseYuX6\nR3s6JlJWZL3PIegt0TrKgxsXuVZEtovI9rGxsTq7aDSDtC9nvS/z6lWLfV/aPvybobw0O3PXlect\nqPvciaOTjIwWUwXG817v/DPm1xVzqZUkDZWelLBgeiYgGqPeHdK/EpETVfWwozZ6ySk/CHjflJOB\nQ075B2rKf+KUnxxQPxBVvQ24DSo2hzr7bmTMyGiR1fftmvGHd3MBQ3g2NW92suJ4ies27uS6jTt9\neulatv/yZV8Ih2mnPOycejN3JQ3tfP4Z8ynk+2a8isIQ4Niaekcmyqx9YDe3fLyyCStul3KtMfr+\nHcWWp2pwYyoljZekVIz1QVUVWLTmIYYGCix6R4Gtzx+J9ABzd1sHrUK+cO8urt+40+wZdZLIldWx\nOTzoMUhvAP7VY5Cep6p/ISKXAJ/jLYP0N1T1XMcgvQN4r9PkU1QM0i+LyJNUjNjbqBik/5uqbo7r\nkxmk24dlX344MCn8YH++Kiiai5uaMowo76Mw46/XSFtLPR5OYecEhVnIirggeC/WhNhoB2P0ytPm\n8eSLRzg6C7mfgyjkc4k+iyw373U6mRmkReRuKrP+E0TkILAOWA/cKyKfAfYDlzvVN1MRDPuouLL+\nEYAjBP4KeNKp92XXOA38KW+5sv4IM0Z3HEGCIao8Lvicq3sOemnDZqZRM9a4wHnelcCAk2gmqI9u\naOcsookGEdXqUMAqpx2M0Y8/93J8pSaSVEiXylNct3FnlQA2I3g0SbyVrgw59MGAugp8NqSd24Hb\nA8q3A++J64fRW4QNfGGqizijcFgwvNoVQphAc2mWYIijOF7itLWbufK8BTNqpUazu/UitZ9e1ESk\n17HAe11CK701BgrBCWTCypMknAmzB4QZf1ecOljX/SdJAeql2dnQophS5c6t+7lhpDLbrTe7m1FN\n7UTEPJ8qmHDoAlrtrXHjR5f4Imzm+4QbP7oksP4lv3NiZHtRUVFvHj6Lq1csnBmkcyKsPG0eT+1/\npa77T6uaOeFt6TKpNYM7t+5n5fpHAWYiiRr1452ItPpdaicstlIXUO+u4SxJs9s1ypAa560UdN0v\n3LsrUN0Tdf/eWEidzNycUJ5SnxeUkYxax4R2eJeaTWYGaaP9abYffxLSJMOJ0pO7L2ASYePO8tKm\neAyLhdSJuF5CJhiCOf2dx3HwyBtVqkPXKB00EWmHd6ldMOHQBdTrx58lUYN57bGwgHSuqihpuIs4\ne4F7/7XX78UXvVc5eOSNVIEA2+FdahdMOHQBcakRm03UYA7+GDphTKmyKCSVZpBXSdQgn+8TVq9a\nHNi3einkc5w8eCzPvvR63W0Ys0upPMVjz4wlVgm1+l1qJ8wg3QXEpUZsNlFxctJ6A0XhCgPXmyTK\nWuAqWbK6vvtMx1492lA7V69Y2HBfjHSkWSm2+l1qJ8wgbTTMKWseChyoXf+lrL5hro44ad6Dwf48\n4xPlhq9fyPdxbD4XuwdisD8fW+fWK5Zy3cadDfaoN4jbMZ6UgUKenev8O/WT0I1hxc0gHUA3ftDt\nQJyeNujYQCHPccfM4ZDjMhiHu7RPsxI4MlFmKKRvx83N8UZ5OpG3Uqk8HR83SWD0Ly+MDWlhgiE5\nWU0qXj86yQ0ju1MnIGp2qPfaa7Xb2NQzaiXzX24eQZux3MH8/DPmB57zkbNP5PE1F/DC+ksiN5bV\nLu3TGpNXr1ocuAfjKx87i+duuZiVp81L1V4YqpUV1PhEY2onI3vKU8pdW/enfvebHerdpV3Hpp4R\nDrP1QXcTUTtFvcc2bNnLJ84ZCtTTPvZMcGj1u7cdmGkvbNfz1SsW8sL6S3h8zQUzs6jjQ3ZdB+Hu\n0K5dHXj/zjI2kEJDaTd7CRFmdXd3WNiMKGbLrbVdx6aeUSuZ/3I60nog3b+jGGi4C3u+U6oz7bmx\ngtxNaW54ZrfcS9LoFe4O7Rs37fG5zU4r3LhpT8uX7b2MamV394Yte1sWHyru3Z8tt9Z2HZt6ZuVQ\nTwaxXiatB1LYTCfq+XrPuXl0XyFnAAATbUlEQVS4ouZ5cf0lPHfLxYGCAWA8wuDrXblsuPxshpcN\nhUaAHS+VWfblh0PbMprP9l++zONrLuDWK5ZmtooY7M/72gqbT8S9+1Hq0ixp17GpZ4TDbH3Q3ULU\nbCbNTCcuOFza2VHYC+OGN6hVQ0UR51lkNJc7t+5nyV/+E1BZRSQJahhVQ4B1ly7xuaJetWJhXe/+\nbLm1tuvY1DNqpbiY/kY1YUtqJTxsdp8Ip6x5qOrZus83LP5R2tnR6lWLq7LOAeRzEvoiJXEvNVrH\n60creRauXrGQ6RjPsTjXVuWt97z2vQ7KIJfk3U8TFqZe2nVs6hnhALPzQXcLUfsJ4hLu1Lr8uc88\nq52nUzVGhPKUhqYJXXfpEp8waYSsfO+NapLEuop77rUh4tvRPTSMdhybekatZKTDu6SuB689wX1J\nS+WpGdVBvUv0m37oNzBDZXAJcv0bXjbEhk+eXaUaCMszEcdAIZ/KWyoN/Xl7FRvFq5VqV/fQTsJ2\nSBuxhO2AjkOAr12xNHX+5iBcARPl2ZI0rPLIaLGu1USuTzgmJ0w0IQJqmKrOSI4ALzh5tnsh9Ha9\nJN0hbdMVI5Z6vSZOGigk8myKy7zlnQVGkdS47a4mkmSk8zI1rU0RDNC69KPdhPd72q7uoZ1ET9kc\njPpIE8/IpU8q510fEi6iOF4KjMBaHC9x3cadXLdx58x+h8eeGUt07T4RRkaLkSuSWj30Jb9zYtfk\nduhlau1XFnq7cUw4GLG4g+1NP9yT2PNnWit+7AMNeAu5OZPT1I+KfRO0sc8EQ+czNFDg/DPms2HL\nXq7fuJOTnL/v31FM5AAxMlqs+m4PFPLc+NElgd8hbwZBd/JSrydUu9OQzUFEXgReBaaASVVdLiLz\ngI3AIuBF4PdU9YiICPB14GJgAvhDVX3Kaeca4Aan2ZtV9Y64a5vNoTUs+/LDiQf7nAhvP3ZO6Ea0\nZlGrV05irzDah6T2F9dzbLA/z2tvTFL2eCoU8rmZJD/F8dJMm7XZ38LsT/k+mdlI6RKWQbA2eVU9\nNrXZZDZtDuer6lLPxdYAP1bV04EfO38DfBg43fm5FviW09F5wDrgPOBcYJ2IDGbQL6MJRO1QrmVK\nlVdmWTBAtV45qb3CaB+mVMn1xW+Ic8fjIxPlKsEAbyX5cTeY1bpZu3atDVv2BjomlKfVt+P/7m0H\nAvtR6z2XRVykODvcbNAMtdJlwAec3+8AfgJ80Sn/nlaWKltFZEBETnTqPqKqLwOIyCPARcDdTehb\nV9EKP+4wXW4QkrJ+Vng3473+5mRmyYaM2WNqWjlubq6hQIaHxkuRDhFxUX5rj6VxGmjE8D2bocKj\naHTloMDDIrJDRK51yt6lqocBnP/f6ZQPAV7Re9ApCyvvOdLMFkZGi6z+/q4qP+7rNu5kUQYzjah+\nxIXD8KIQGrK7mUypzjyT2VZpGdkx0WCE26h84W55lIG69liS8B5h56ahXaK0NiocVqrqe6mojD4r\nIu+PqBv0ZDWi3N+AyLUisl1Eto+NBYeC7lTSbtq5cdMe31LapZENP3H9CIo3E8WDuw6n7oNhQOOe\nRRNHJxkIcVce6M9HJmZyc5B7CQstX6sBazQuUru44TakVlLVQ87/L4nIP1KxGfxKRE5U1cOO2ugl\np/pBwPt0TwYOOeUfqCn/Scj1bgNug4pBupG+txtxy99a4mbE3pmGq3oa6M+jCq+UyqFqqLB+3PTD\nPaEqrNPWbg5dctvM3aiX/rl9icOV5HPCnD6pyth3ZKJMvk/I58RnVzgyUQ51rAjzVgoLLZ+1t1K7\nuOHWLRxE5DigT1VfdX6/EPgysAm4Bljv/P8D55RNwOdE5B4qxudXHAGyBfhrjxH6QmBtvf3qVJox\nW3Bn/e5g730ZwvSYYTMp78tUe+6V5y0wl1Ajc5596fVE9Qb786y7dEmgR1p5WmdS0hbHS5HCJsnu\n6ZuHzwoMJ5+lLSBoX1ErorQ2olZ6F/B/RGQX8ATwkKr+ExWh8CEReRb4kPM3wGbgeWAf8G3gPwI4\nhui/Ap50fr7sGqd7ibQx3ZPs7s2JRBpjg/SYSdWq3nOXv3uebbU3Wkb/3DmRxuVXSmUeX3MBQwOF\nyFVIu+yenq1Q4XHUvXJQ1eeBswPK/xX4YEC5Ap8Naet24PZ6+9INpJ0txEUbLeRzibx0al+INNte\n3Fnahi17aU5QCcOIx2tcjlLHJMn81i60Q5TWnprwtYPvcBhpZwu10UYHCnkG+/NV5yaJqKpQ97Nw\nvTfqnXHdesVSXlx/CbdesZR8Ar/2nEhkshejN3G/w4veEfx9dz3m4gb/iaOToe9BO48dzaJnwme0\ni+9wFI3MFo47Zo5v5+fE0clE53qfRZrkOK4R+vhCPrXhebA/X32vMaN+H6Co5VIwAimOl0LtZQ/u\nOszNw2fFxgg7MlEOHBM6YexoBj2zcmgX3+GsiHI5dY/VDvL9+b7QXAbus1h36ZJEu1O9/QizU/QJ\nXL1ioa+9XJ+w7tIlM7Ox6zbujAyfLcA0/p2ohpGE8VKZlesfBeLTkQaNCWFjx3Ubd3b1KqJnhEO7\n+A5nRZSwCzoGMHjcMexcd2Fom+7MK82XIkgIuagGG6v7qATlSxLWotHE8znTQxlUz/bjdjrXjglR\nY0Q3JxHqGeGQ1huo3YkSdnGCMGzmlBOpxJpJMUX3Znerxc3nUNteeVq5a+v+RAbzUnmqIVVSRtlB\njS7AnTzF7XSuHRPixohO1kBE0TPCISjsQyt8h7MiaudnnCCMygFdTxykKdXQZxvWno3ZRisojpci\nVw7eMeGGkd2ctnZzoneiUzUQUfSMcGgX3+GsCPt+q8YLwnrzQofh9Y7qhmdrdC9C9F6eT5xTcQpx\nw3MnDbbXqRqIKHrGWwnaw3c4K8JCYb9SKs/cY9iW/noyu4XhCp1uerZG96Iz/wTz0M8O8+Cuw6He\ndwIcW7OHqBkaCG8OkrBcFM2mp4RDtzAyWqQvJCGKO4OJGqxrhUe9Kp6ciK0QjK4izo1bgWPm9M0I\nBzd0R5bvQK3rbG0uCpgdF1oTDh2CdyYRFh8mzQzGKzyiolO61F6z3bNdGUaz8K4q3ihnHxsgzNsQ\nooNxZk3P2Bw6mdpsZkGCoZFZfFyOhkI+x1UrFnakTaGQz9Gft6+54ScLL+dmeCrFGbdny/htK4cO\nIGom4TKtmmqwrs0i5+bbTRrau50YKOR5/eikbyOdG3r5+o07W9QzIy2us0RW2QNFgp03XD2+V5//\n+puTdYWYz3qwjsueOFvGbxMOHUCSL1+aL0xQOID7dxQ7ZjXgpZDP8ZGzT2TjE9X5ffN9MhOT/6Yf\n7kkcEsQIZ2igwPlnzGfjkwcid7TXSx+V+EZZfVaFfI5PnDPE/TuKvsmVq8d33bBddezq7+8K3ecT\nps7NerCOchiZTfd7W293AHFfvrRfmKxCiSQJRpbUbTbXJ4nDhYvz46q3HntmLHCjnXs/aSLNGuEU\nx0vcuXV/UwQDVEKkZCcY+jg238ddW/dzbETYGKjW42+4/OzAugOFPFetWDgre6W8bvfw1qbV2Vbn\n2sqhAwiaSbizmHrc28KWrGmW8kmDkUX1vdZFD0jkYvu1K5ZWXSNMbeSuuMLcfo3OQKgEdwxSHYZR\nKk/PZIWLygjn4n5X3O9V7ffwzclplr97XqKsb7Uq23rUsu3gGm7CoQOI27eQllyIG2yaBOpJ05rW\n0/coNZA7q1u5/tGZ9sKiwrorrjgdrtHevLD+Es78Lz9qaMUSFxLGuzqP+m4/vuYCX8RW73fx/DPm\nV6mxOjmCqwmHDiHLmURU+IykpAlkmKbvw8uG2LBlb6BwEGDJSW/n+o07Z3S/3mCB0zV13Tj+558x\n39KYdjBXffunTDTBZdQl3ydVqqGo77Z3VVC7mimOl7hr636fXWI23U+zxIRDDzIUMpOOsg/ULpXj\nZuuNLK3DXk4FHn8uOINs7dChwF2OQHjsmbFE1zXaDyH8M8+Ktx07p+q7GbbSHOjPV6mbgr7/YdOr\nToy9JNqh1rrly5fr9u3bW92NjmRktOhLMZrPCRs+eXbgAF5rX3Dro9XLdXdjHPh1tq7niOsuWysw\nvMIkbPe30XuEeQhlzdUrFnLzcOW7G/R9L+RzTE5NUe8CZmigwONrLsiiqw0jIjtUdXlcPVs5dDlB\nM3jA/8ZFvIFBOtjylDLYn6d/7hzfYL9y/aOBOlvvkturiwUCwwUYxmx9E+7cup9/fKrIVz52VqCd\nbNE7CnWvYDo1+rOtHCLIwuuglYTNgI6Z0xe4JA6b3Zyy5qHAl1SoGAvda9UbqynKQD6tlhrUmF28\n8ZK8YWvS4N18FxV/qRVjTNKVQ9sIBxG5CPg6kAO+o6rro+o3WziEDaydtFEsScykWl50BnsvS296\nOFCYDBTy7Fx3YeCzyooX11/CqWsfshShRldQ7bb9sxl3Wy/uzn6oXr2cf8b8ULVsGjpKrSQiOeCb\nwIeAg8CTIrJJVX/Rqj4lddVsZ7IygoV5uLrlScJ71KM7zokwMlo0wWB0DcXxEqvv28XUlPqcKFzG\nS2VWf38XCFWeUF6Pu9lwkW2XHdLnAvtU9XlVPQrcA1zWyg51Q87prLb1j4fsOXDLo56Ju5M5aHdp\nHFOqVXYJw+gGyhGCYabOtMbu62h2etJ2EQ5DgDc4zkGnrGV0Q87psIxwYSuBqFzQUeVhx4cGCryw\n/hIeX3MBNw+fVRUSIAk5kaaoqgyjW2jmZLVdhEPQqOQTmyJyrYhsF5HtY2PN9V3vhpzTYalRrzpv\nYWD9K89bEFge9yySPqvhZUM8vuaCRKGSC/ncrHstDQ0UuPWKpZmnUTWMZtHMyWq7CIeDgHdkOhk4\nVFtJVW9T1eWqunz+/PlN7VC35Jx2B2R3Bj+8bIibh8/i6hULZ1YKOZEqP++gNqKeRdpnddWKYOF0\n3Nxc1flJB+mVp82rGtTd+xoo5BnsDw+45iWfkxkD3+NrLgi9dlgAt8H+/Ezfr3ZyXxiN8a63z211\nF5pCPiexA68bCyqKZk9W28JbSUTmAP8CfBAoAk8Cv6+qe8LOsU1wnc0NI7u5e9uBmZj6V563wCec\norygGvEcGxktcuOmPTMeWEGuhlHeapA8VlStq6LrceLNDTxQyCNSCRDXJ1QZ4PsETpt/HM+PTfie\n1choMTTE9NUrFgYGidv+y5cDn7u3n7X5PLxeMv1zc0wcnZoJnLji1EFG949nHt5i5WnzuOuP38dV\n3/5p4P6C0995HBNHp6ue693b9lOrpi/k+3hzcjozp4b+fB+l8jRz+gjdEBflfOH1VvJ+B73neeu0\n0lupLYQDgIhcDNxKxZX1dlX9SlR9Ew69QSsTrXfCPpckgq5V/QoTNkEDXxoBm/ZzCNsIWhvgMctn\n187fnY4TDmkx4WAYhpGepMKhXWwOhmEYRhthwsEwDMPwYcLBMAzD8GHCwTAMw/BhwsEwDMPw0bHe\nSiIyBvyygSZOAH6dUXfanV65V7vP7sLuszm8W1VjdxF3rHBoFBHZnsSdqxvolXu1++wu7D5bi6mV\nDMMwDB8mHAzDMAwfvSwcbmt1B2aRXrlXu8/uwu6zhfSszcEwDMMIp5dXDoZhGEYIPSkcROQiEdkr\nIvtEZE2r+9MMRGSBiDwmIk+LyB4R+Xyr+9RMRCQnIqMi8mCr+9IsRGRARO4TkWecz/V9re5TMxCR\n653v7M9F5G4RObbVfcoKEbldRF4SkZ97yuaJyCMi8qzz/2Ar++jSc8JBRHLAN4EPA2cCV4rIma3t\nVVOYBL6gqr8NrAA+26X36fJ54OlWd6LJfB34J1U9AzibLrxfERkC/hxYrqrvoRLC/1Ot7VWm/ANw\nUU3ZGuDHqno68GPn75bTc8IBOBfYp6rPq+pR4B7gshb3KXNU9bCqPuX8/iqVgaQ9AspnjIicDFwC\nfKfVfWkWIvJbwPuB7wKo6lFVHW9tr5rGHKDgJAHrJyArZKeiqv8M1GYvugy4w/n9DmB4VjsVQi8K\nhyHggOfvg3TpoOkiIouAZcC21vakadwK/AWQbTqy9uJUYAz4H4767DsiclyrO5U1qloE/hbYDxwG\nXlHVh1vbq6bzLlU9DJVJHfDOFvcH6E3hEJSYtWtdtkTkbcD9wHWq+ptW9ydrROQjwEuquqPVfWky\nc4D3At9S1WXA67SJ+iFLHH37ZcApwEnAcSJydWt71Zv0onA4CCzw/H0yXbRs9SIieSqC4S5VfaDV\n/WkSK4GPisiLVFSEF4jIna3tUlM4CBxUVXf1dx8VYdFt/C7wgqqOqWoZeAD49y3uU7P5lYicCOD8\n/1KL+wP0pnB4EjhdRE4RkblUjF2bWtynzBERoaKfflpVv9rq/jQLVV2rqier6iIqn+Wjqtp1M01V\n/X/AARFZ7BR9EPhFC7vULPYDK0Sk3/kOf5AuNLzXsAm4xvn9GuAHLezLDHNa3YHZRlUnReRzwBYq\nnhC3q+qeFnerGawEPg3sFpGdTtmXVHVzC/tkNMafAXc5k5rngT9qcX8yR1W3ich9wFNUPO5GadMd\nxPUgIncDHwBOEJGDwDpgPXCviHyGinC8vHU9fAvbIW0YhmH46EW1kmEYhhGDCQfDMAzDhwkHwzAM\nw4cJB8MwDMOHCQfDMAzDhwkHwzAMw4cJB8MwDMOHCQfDMAzDx/8H3UW3SXdY/JEAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f057d3fae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "plt.scatter(df['crop_sales___output___ethiopia_2015'], df['weight___ethiopia_2015'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>children_education___output___ethiopia_2011</th>\n",
       "      <th>crop_sales___output___ethiopia_2011</th>\n",
       "      <th>expenditure___output___ethiopia_2011</th>\n",
       "      <th>food_expenditure_diversification___output___ethiopia_2011</th>\n",
       "      <th>has_medical_assistance___output___ethiopia_2011</th>\n",
       "      <th>no_food_deficiency___output___ethiopia_2011</th>\n",
       "      <th>productivity___output___ethiopia_2011</th>\n",
       "      <th>amount_of_assistance_received___policy___ethiopia_2011</th>\n",
       "      <th>crop_diversification___policy___ethiopia_2011</th>\n",
       "      <th>...</th>\n",
       "      <th>longitude___ethiopia_2015</th>\n",
       "      <th>number_of_droughts___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_mainly_non-soil___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_moderate_constraint___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_no_or_slight_constraint___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_severe_constraint___ethiopia_2015</th>\n",
       "      <th>rooting_conditions_:_very_severe_constraint___ethiopia_2015</th>\n",
       "      <th>rural_household___ethiopia_2015</th>\n",
       "      <th>variations_in_greenness___ethiopia_2015</th>\n",
       "      <th>weight___ethiopia_2015</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3639.000000</td>\n",
       "      <td>2549.000000</td>\n",
       "      <td>1779.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3476.000000</td>\n",
       "      <td>3612.000000</td>\n",
       "      <td>3601.000000</td>\n",
       "      <td>1731.000000</td>\n",
       "      <td>3606.000000</td>\n",
       "      <td>2702.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "      <td>3639.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1819.000000</td>\n",
       "      <td>0.656267</td>\n",
       "      <td>6.239407</td>\n",
       "      <td>6.658564</td>\n",
       "      <td>0.664381</td>\n",
       "      <td>0.266112</td>\n",
       "      <td>0.714524</td>\n",
       "      <td>-2.744503</td>\n",
       "      <td>0.333306</td>\n",
       "      <td>2.556995</td>\n",
       "      <td>...</td>\n",
       "      <td>38.454400</td>\n",
       "      <td>0.254937</td>\n",
       "      <td>0.043419</td>\n",
       "      <td>0.154713</td>\n",
       "      <td>0.501786</td>\n",
       "      <td>0.156362</td>\n",
       "      <td>0.142622</td>\n",
       "      <td>0.885408</td>\n",
       "      <td>43.302830</td>\n",
       "      <td>4319.752745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1050.633142</td>\n",
       "      <td>0.366369</td>\n",
       "      <td>1.526063</td>\n",
       "      <td>1.514439</td>\n",
       "      <td>0.254461</td>\n",
       "      <td>0.322030</td>\n",
       "      <td>0.451704</td>\n",
       "      <td>1.761184</td>\n",
       "      <td>1.405252</td>\n",
       "      <td>2.883183</td>\n",
       "      <td>...</td>\n",
       "      <td>2.034458</td>\n",
       "      <td>0.441705</td>\n",
       "      <td>0.203825</td>\n",
       "      <td>0.361680</td>\n",
       "      <td>0.500066</td>\n",
       "      <td>0.363248</td>\n",
       "      <td>0.349735</td>\n",
       "      <td>0.318573</td>\n",
       "      <td>12.656131</td>\n",
       "      <td>4187.333902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406284</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.616763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>33.468357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.844484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>909.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.273818</td>\n",
       "      <td>6.171991</td>\n",
       "      <td>0.532509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.885991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>37.144541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1225.015137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1819.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.292388</td>\n",
       "      <td>6.904063</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.772643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38.311341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>3269.653564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2728.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.250832</td>\n",
       "      <td>7.524839</td>\n",
       "      <td>0.859984</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.652558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.650890</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>6107.384277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3638.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.684777</td>\n",
       "      <td>13.337703</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.189095</td>\n",
       "      <td>10.204448</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.870658</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>32753.236328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  210 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  children_education___output___ethiopia_2011  \\\n",
       "count  3639.000000                                  2549.000000   \n",
       "mean   1819.000000                                     0.656267   \n",
       "std    1050.633142                                     0.366369   \n",
       "min       0.000000                                     0.000000   \n",
       "25%     909.500000                                     0.500000   \n",
       "50%    1819.000000                                     0.750000   \n",
       "75%    2728.500000                                     1.000000   \n",
       "max    3638.000000                                     1.000000   \n",
       "\n",
       "       crop_sales___output___ethiopia_2011  \\\n",
       "count                          1779.000000   \n",
       "mean                              6.239407   \n",
       "std                               1.526063   \n",
       "min                               0.406284   \n",
       "25%                               5.273818   \n",
       "50%                               6.292388   \n",
       "75%                               7.250832   \n",
       "max                              10.684777   \n",
       "\n",
       "       expenditure___output___ethiopia_2011  \\\n",
       "count                           3639.000000   \n",
       "mean                               6.658564   \n",
       "std                                1.514439   \n",
       "min                                0.000000   \n",
       "25%                                6.171991   \n",
       "50%                                6.904063   \n",
       "75%                                7.524839   \n",
       "max                               13.337703   \n",
       "\n",
       "       food_expenditure_diversification___output___ethiopia_2011  \\\n",
       "count                                        3476.000000           \n",
       "mean                                            0.664381           \n",
       "std                                             0.254461           \n",
       "min                                             0.000000           \n",
       "25%                                             0.532509           \n",
       "50%                                             0.750000           \n",
       "75%                                             0.859984           \n",
       "max                                             1.000000           \n",
       "\n",
       "       has_medical_assistance___output___ethiopia_2011  \\\n",
       "count                                      3612.000000   \n",
       "mean                                          0.266112   \n",
       "std                                           0.322030   \n",
       "min                                           0.000000   \n",
       "25%                                           0.000000   \n",
       "50%                                           0.166667   \n",
       "75%                                           0.428571   \n",
       "max                                           1.000000   \n",
       "\n",
       "       no_food_deficiency___output___ethiopia_2011  \\\n",
       "count                                  3601.000000   \n",
       "mean                                      0.714524   \n",
       "std                                       0.451704   \n",
       "min                                       0.000000   \n",
       "25%                                       0.000000   \n",
       "50%                                       1.000000   \n",
       "75%                                       1.000000   \n",
       "max                                       1.000000   \n",
       "\n",
       "       productivity___output___ethiopia_2011  \\\n",
       "count                            1731.000000   \n",
       "mean                               -2.744503   \n",
       "std                                 1.761184   \n",
       "min                                -8.616763   \n",
       "25%                                -3.885991   \n",
       "50%                                -2.772643   \n",
       "75%                                -1.652558   \n",
       "max                                 4.189095   \n",
       "\n",
       "       amount_of_assistance_received___policy___ethiopia_2011  \\\n",
       "count                                        3606.000000        \n",
       "mean                                            0.333306        \n",
       "std                                             1.405252        \n",
       "min                                             0.000000        \n",
       "25%                                             0.000000        \n",
       "50%                                             0.000000        \n",
       "75%                                             0.000000        \n",
       "max                                            10.204448        \n",
       "\n",
       "       crop_diversification___policy___ethiopia_2011           ...            \\\n",
       "count                                    2702.000000           ...             \n",
       "mean                                        2.556995           ...             \n",
       "std                                         2.883183           ...             \n",
       "min                                         0.000000           ...             \n",
       "25%                                         0.000000           ...             \n",
       "50%                                         2.000000           ...             \n",
       "75%                                         4.000000           ...             \n",
       "max                                        16.000000           ...             \n",
       "\n",
       "       longitude___ethiopia_2015  number_of_droughts___ethiopia_2015  \\\n",
       "count                3639.000000                         3639.000000   \n",
       "mean                   38.454400                            0.254937   \n",
       "std                     2.034458                            0.441705   \n",
       "min                    33.468357                            0.000000   \n",
       "25%                    37.144541                            0.000000   \n",
       "50%                    38.311341                            0.000000   \n",
       "75%                    39.650890                            0.693147   \n",
       "max                    43.870658                            2.944439   \n",
       "\n",
       "       rooting_conditions_:_mainly_non-soil___ethiopia_2015  \\\n",
       "count                                        3639.000000      \n",
       "mean                                            0.043419      \n",
       "std                                             0.203825      \n",
       "min                                             0.000000      \n",
       "25%                                             0.000000      \n",
       "50%                                             0.000000      \n",
       "75%                                             0.000000      \n",
       "max                                             1.000000      \n",
       "\n",
       "       rooting_conditions_:_moderate_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000          \n",
       "mean                                            0.154713          \n",
       "std                                             0.361680          \n",
       "min                                             0.000000          \n",
       "25%                                             0.000000          \n",
       "50%                                             0.000000          \n",
       "75%                                             0.000000          \n",
       "max                                             1.000000          \n",
       "\n",
       "       rooting_conditions_:_no_or_slight_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000              \n",
       "mean                                            0.501786              \n",
       "std                                             0.500066              \n",
       "min                                             0.000000              \n",
       "25%                                             0.000000              \n",
       "50%                                             1.000000              \n",
       "75%                                             1.000000              \n",
       "max                                             1.000000              \n",
       "\n",
       "       rooting_conditions_:_severe_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000        \n",
       "mean                                            0.156362        \n",
       "std                                             0.363248        \n",
       "min                                             0.000000        \n",
       "25%                                             0.000000        \n",
       "50%                                             0.000000        \n",
       "75%                                             0.000000        \n",
       "max                                             1.000000        \n",
       "\n",
       "       rooting_conditions_:_very_severe_constraint___ethiopia_2015  \\\n",
       "count                                        3639.000000             \n",
       "mean                                            0.142622             \n",
       "std                                             0.349735             \n",
       "min                                             0.000000             \n",
       "25%                                             0.000000             \n",
       "50%                                             0.000000             \n",
       "75%                                             0.000000             \n",
       "max                                             1.000000             \n",
       "\n",
       "       rural_household___ethiopia_2015  \\\n",
       "count                      3639.000000   \n",
       "mean                          0.885408   \n",
       "std                           0.318573   \n",
       "min                           0.000000   \n",
       "25%                           1.000000   \n",
       "50%                           1.000000   \n",
       "75%                           1.000000   \n",
       "max                           1.000000   \n",
       "\n",
       "       variations_in_greenness___ethiopia_2015  weight___ethiopia_2015  \n",
       "count                              3639.000000             3639.000000  \n",
       "mean                                 43.302830             4319.752745  \n",
       "std                                  12.656131             4187.333902  \n",
       "min                                  12.000000                6.844484  \n",
       "25%                                  36.000000             1225.015137  \n",
       "50%                                  44.000000             3269.653564  \n",
       "75%                                  51.000000             6107.384277  \n",
       "max                                  68.000000            32753.236328  \n",
       "\n",
       "[8 rows x 210 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/ananth/Downloads/ethiopia_v23_normed.csv')\n",
    "df.describe()\n",
    "# df = df.loc[df['crop_sales___output___norm___ethiopia_2015'].dropna().index]\n",
    "# r = np.mean(df)\n",
    "# rdict = r.to_dict()\n",
    "# rdict['land_surface___norm___ethiopia_2015']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amount_of_assistance_received___policy',\n",
       " 'attended_school',\n",
       " 'average_precipitation',\n",
       " 'average_temperature',\n",
       " 'children_education___output',\n",
       " 'crop_diversification___policy',\n",
       " 'crop_sales___output',\n",
       " 'distance_to_market',\n",
       " 'distance_to_population_center',\n",
       " 'distance_to_road',\n",
       " 'elevation',\n",
       " 'expenditure___output',\n",
       " 'food_expenditure_diversification___output',\n",
       " 'has_borrowed___policy',\n",
       " 'has_medical_assistance___output',\n",
       " 'heavy_rains_preventing_work',\n",
       " 'household_head_age',\n",
       " 'household_head_is_divorced',\n",
       " 'household_head_is_female',\n",
       " 'household_head_is_male',\n",
       " 'household_head_is_monogamous',\n",
       " 'household_head_is_polygamous',\n",
       " 'household_head_is_separated',\n",
       " 'household_head_is_widowed',\n",
       " 'household_head_never_married',\n",
       " 'household_size',\n",
       " 'illness_of_household_member___policy',\n",
       " 'increase_in_price_of_inputs___policy',\n",
       " 'land_surface',\n",
       " 'latitude',\n",
       " 'literacy',\n",
       " 'lives_in_afar',\n",
       " 'lives_in_amhara',\n",
       " 'lives_in_benishangul_gumuz',\n",
       " 'lives_in_dire_dawa',\n",
       " 'lives_in_gambella',\n",
       " 'lives_in_harari',\n",
       " 'lives_in_oromiya',\n",
       " 'lives_in_snnp',\n",
       " 'lives_in_somalie',\n",
       " 'lives_in_tigray',\n",
       " 'longitude',\n",
       " 'no_food_deficiency___output',\n",
       " 'number_of_axe_owned___policy',\n",
       " 'number_of_droughts',\n",
       " 'number_of_hired_workers___policy',\n",
       " 'number_of_oxen_owned___policy',\n",
       " 'number_of_pick_axe_owned___policy',\n",
       " 'number_of_plough_owned___policy',\n",
       " 'number_of_sickle_owned___policy',\n",
       " 'number_of_water_storage_pit_owned___policy',\n",
       " 'owns_land_certificate___policy',\n",
       " 'percentage_of_damaged_crop___policy',\n",
       " 'prevent_damage___policy',\n",
       " 'price_rise_of_food_item___policy',\n",
       " 'productivity___output',\n",
       " 'quantity_of_chemical_fertilizers_used___policy',\n",
       " 'quantity_of_improved_seeds_used___policy',\n",
       " 'rooting_conditions_:_mainly_non-soil',\n",
       " 'rooting_conditions_:_moderate_constraint',\n",
       " 'rooting_conditions_:_no_or_slight_constraint',\n",
       " 'rooting_conditions_:_severe_constraint',\n",
       " 'rooting_conditions_:_very_severe_constraint',\n",
       " 'rural_household',\n",
       " 'uses_credit___policy',\n",
       " 'uses_extension_program___policy',\n",
       " 'uses_irrigation___policy',\n",
       " 'variations_in_greenness',\n",
       " 'weight'}"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_df = read_csv('./ethiopia_v23_raw.csv')\n",
    "imp_cols = imp_df.columns.values\n",
    "imp_feats = set([])\n",
    "for y in [2011, 2013, 2015]:\n",
    "    y_reg = re.compile('.*___.*' + str(y) + '$')\n",
    "    y_cols = set(filter(y_reg.search, imp_cols))\n",
    "    y_cols = set([t.replace('___ethiopia_' + str(y), '') for t in y_cols])\n",
    "    if len(imp_feats) == 0:\n",
    "        imp_feats = y_cols\n",
    "    else:\n",
    "        imp_feats = imp_feats.intersection(y_cols)\n",
    "imp_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['owns_land_certificate___policy___ethiopia_2015', 'lives_in_dire_dawa___ethiopia_2015', 'price_rise_of_food_item___policy___ethiopia_2015', 'longitude___ethiopia_2015', 'lives_in_oromiya___ethiopia_2015', 'rooting_conditions_:_no_or_slight_constraint___ethiopia_2015', 'percentage_of_damaged_crop___policy___ethiopia_2015', 'household_head_is_separated___ethiopia_2015', 'lives_in_gambella___ethiopia_2015', 'number_of_axe_owned___policy___ethiopia_2015', 'household_head_age___ethiopia_2015', 'latitude___ethiopia_2015', 'household_head_is_monogamous___ethiopia_2015', 'productivity___output___ethiopia_2015', 'number_of_pick_axe_owned___policy___ethiopia_2015', 'number_of_hired_workers___policy___ethiopia_2015', 'number_of_water_storage_pit_owned___policy___ethiopia_2015', 'uses_extension_program___policy___ethiopia_2015', 'number_of_oxen_owned___policy___ethiopia_2015', 'household_head_is_male___ethiopia_2015', 'household_size___ethiopia_2015', 'rooting_conditions_:_severe_constraint___ethiopia_2015', 'household_head_is_widowed___ethiopia_2015', 'rural_household___ethiopia_2015', 'illness_of_household_member___policy___ethiopia_2015', 'distance_to_road___ethiopia_2015', 'food_expenditure_diversification___output___ethiopia_2015', 'attended_school___ethiopia_2015', 'amount_of_assistance_received___policy___ethiopia_2015', 'lives_in_benishangul_gumuz___ethiopia_2015', 'household_head_is_divorced___ethiopia_2015', 'lives_in_afar___ethiopia_2015', 'literacy___ethiopia_2015', 'number_of_droughts___ethiopia_2015', 'increase_in_price_of_inputs___policy___ethiopia_2015', 'number_of_plough_owned___policy___ethiopia_2015', 'distance_to_population_center___ethiopia_2015', 'crop_diversification___policy___ethiopia_2015', 'weight___ethiopia_2015', 'no_food_deficiency___output___ethiopia_2015', 'rooting_conditions_:_mainly_non-soil___ethiopia_2015', 'household_head_never_married___ethiopia_2015', 'lives_in_tigray___ethiopia_2015', 'heavy_rains_preventing_work___ethiopia_2015', 'variations_in_greenness___ethiopia_2015', 'distance_to_market___ethiopia_2015', 'expenditure___output___ethiopia_2015', 'crop_sales___output___ethiopia_2015', 'lives_in_snnp___ethiopia_2015', 'lives_in_harari___ethiopia_2015', 'elevation___ethiopia_2015', 'prevent_damage___policy___ethiopia_2015', 'land_surface___ethiopia_2015', 'number_of_sickle_owned___policy___ethiopia_2015', 'has_borrowed___policy___ethiopia_2015', 'uses_credit___policy___ethiopia_2015', 'quantity_of_chemical_fertilizers_used___policy___ethiopia_2015', 'rooting_conditions_:_moderate_constraint___ethiopia_2015', 'rooting_conditions_:_very_severe_constraint___ethiopia_2015', 'household_head_is_polygamous___ethiopia_2015', 'quantity_of_improved_seeds_used___policy___ethiopia_2015', 'uses_irrigation___policy___ethiopia_2015', 'average_precipitation___ethiopia_2015', 'children_education___output___ethiopia_2015', 'household_head_is_female___ethiopia_2015', 'lives_in_amhara___ethiopia_2015', 'lives_in_somalie___ethiopia_2015', 'average_temperature___ethiopia_2015', 'has_medical_assistance___output___ethiopia_2015']\n"
     ]
    }
   ],
   "source": [
    "feats = list(imp_feats)\n",
    "feats = [for_year(f, 2015) for f in feats]\n",
    "print(feats)\n",
    "imp_vars = re.compile('^.*(?=improved_seeds|water_storage|saved|extension|plough|oxen|hired_workers|fertilizer|health_issues|sickle|axe|credit).*$')\n",
    "input_vars = list(filter(imp_vars.search, feats))\n",
    "collinear = imp_df[feats].corr('spearman')\n",
    "collinear.to_csv('./output_collinear.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afar 0.0398055198279 2129.3261381\n",
      "amhara 25.8150368658 1155.75359963\n",
      "benishangul_gumuz 1.59543145271 2725.0879217\n",
      "dire_dawa 0.174135908025 1150.03649518\n",
      "gambella 0.31624091125 2110.87379671\n",
      "harari 0.180543438023 8034.89341087\n",
      "oromiya 41.046503324 1767.39513676\n",
      "snnp 26.6685963573 1036.59091902\n",
      "somalie 0.409996338586 1545.2165131\n",
      "tigray 3.75370988447 1898.11291005\n"
     ]
    }
   ],
   "source": [
    "imp_df\n",
    "locations = ['afar', 'amhara', 'benishangul_gumuz', 'dire_dawa', 'gambella', 'harari', 'oromiya', 'snnp', 'somalie', 'tigray']\n",
    "imp_df = imp_df.loc[imp_df[output].dropna().index]\n",
    "for l in sorted(locations):\n",
    "    loc_feature = 'lives_in_' + l + '___ethiopia_' + str(2015)\n",
    "    df_loc = imp_df.loc[imp_df[loc_feature]==1]\n",
    "    output = 'crop_sales___output___ethiopia_2015'  \n",
    "    print (l, sum(df_loc['weight___ethiopia_2015'])/sum(imp_df['weight___ethiopia_2015'])*100.0, np.average(df_loc[output], weights=df_loc['weight___ethiopia_2015']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.4168190128\n",
      "67.0932358318\n",
      "82.449725777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "489.09268211400331"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(imp_df['crop_sales___output___ethiopia_2015'].dropna(), [25,50,75])\n",
    "np.std(imp_df['crop_sales___output___ethiopia_2015'].dropna())\n",
    "from scipy.stats import percentileofscore\n",
    "x = [568.191517737871, 245.304134630863, 902.739225809435]\n",
    "for i in x:\n",
    "    print(percentileofscore(df[output + '_change_value' + year].dropna(), i))\n",
    "np.percentile(df[output + '_change_value' + year].dropna(), 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = ['segment_children_education___output',\n",
    "           'segment_crop_sales___output',\n",
    "           'segment_crop_sales_growth___output',\n",
    "           'segment_expenditure___output',\n",
    "           'segment_food_expenditure_diversification___output']\n",
    "\n",
    "# all_coef = pd.read_excel('./Tables_fixed_diversification/'+'coef_ethiopia.xlsx')\n",
    "\n",
    "df = df_all\n",
    "def for_year(var, year):\n",
    "    return var + '___ethiopia_' + str(year)\n",
    "\n",
    "\n",
    "\n",
    "# ['use_extension_program___policy',\n",
    "#     'prevent_damage___policy',\n",
    "#     'hired_labor___policy', \n",
    "#     'oxen_owned___policy', \n",
    "#     'chemical_fertilizers_used___policy']\n",
    "series = pd.DataFrame()\n",
    "for imp_feat in imp_feats:\n",
    "    for output in ['segment_crop_sales___output']:\n",
    "        years = ['2011', '2013', '2015']\n",
    "        raw_output = 'crop_sales___output'\n",
    "        coef= {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1}\n",
    "        for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "            #try:\n",
    "            z1 = for_year(output, y1)\n",
    "            z2 = for_year(output, y2)\n",
    "            r1 = for_year(raw_output, y1)\n",
    "            r2 = for_year(raw_output, y2)\n",
    "            f1 = for_year(imp_feat,y1)\n",
    "            f2 = for_year(imp_feat,y2)\n",
    "            df[output + '_change' + y1] = (df[z1]!=df[z2])\n",
    "            df[output + '_increase' + y1] = (df[z1]<df[z2])\n",
    "            df[output + '_decrease' + y1] = (df[z1]>df[z2])\n",
    "            expected = df[z1].apply(lambda x: coef[x])\n",
    "            df[imp_feat+'_increase'+y1] = (imp_df[f1]<imp_df[f2])\n",
    "            df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*expected\n",
    "            df[output + '_change_value' + y1] = (imp_df[r2]-imp_df[r1])\n",
    "            \n",
    "\n",
    "    # for output in ['segment_crop_sales___output']:\n",
    "    #     imp_feat = {'2011': 'distance_to_market___policy', '2013': 'prevent_damage___policy', '2015': 'hired_labor___policy'} \n",
    "    #     #imp_feat = {'2011': 'chemical_fertilizers_used___policy', '2013': 'chemical_fertilizers_used___policy', '2015': 'damaged_crop___policy'} \n",
    "    #     coef = {'2011': -0.168, '2013':+0.184, '2015': +0.182}\n",
    "    #     segment = 'hired_labor___policy'\n",
    "    #     df[output + '_change'] = False\n",
    "    #     from itertools import combinations\n",
    "    #     years = ['2011', '2013', '2015']\n",
    "    #     for y in years:\n",
    "    #         y_segment = for_year(segment, y)\n",
    "    #         df = df.loc[df[y_segment].dropna().index]\n",
    "    #         median_s = np.median(df[y_segment])\n",
    "    #         df['cluster'+y] = [int(x) for x in df[y_segment] > median_s]\n",
    "    #     df = df[df['cluster2011']==0]\n",
    "    #     for [y1, y2] in combinations(years, 2):\n",
    "    #         #try:\n",
    "    #         z1 = for_year(output, y1)\n",
    "    #         z2 = for_year(output, y2)\n",
    "    #         f1 = for_year(imp_feat[y1],y1)\n",
    "    #         f2 = for_year(imp_feat[y2],y2)\n",
    "    #         df[output + '_change' + y1] = ((df[z1]!=df[z2]) & (df[z1]!=6) & (df[z2]!=6))\n",
    "    #         df[output + '_inversion' + y1] = (df[z2]-df[z1])*(df[f2]-df[f1])*coef[y1]\n",
    "    # #         except:\n",
    "    # #             continue\n",
    "    #         year = y1\n",
    "        \n",
    "        thresholds = {'2011': {0: 582.074037761, 1: 932.333779703, 2: 585.830244031, 3: 1026.46232607},\n",
    "                      '2013': {0: 466.006285787, 1: 539.93544304, 2: 350.635222524, 3: 830.09513343}}\n",
    "#         thresholds = [489.1]*4\n",
    "#         df = df[df[output].dropna().index]\n",
    "        for per in [50,55,60,70,75,80,85,90,95]:\n",
    "            for seg in range(4):\n",
    "                exp_y_i = []\n",
    "                exp_y = []\n",
    "                exp = []\n",
    "                exp_i = []\n",
    "                for [y1, y2] in [['2011', '2013'], ['2013', '2015']]:\n",
    "                    year = y1\n",
    "                    weight = for_year('weight', year)\n",
    "    #                 thres = thresholds[year][seg]\n",
    "                    seg_y = for_year(output, year)\n",
    "                    df_seg = df[df[seg_y]==seg]\n",
    "                    df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "                    thres = np.percentile(df_seg[output + '_change_value' + year].dropna(), per)\n",
    "    # #                 print (min(df_seg[output + '_change_value' + year].abs()))\n",
    "    #                 df_x = df_seg[(df_seg[output + '_change'+year]==True)]\n",
    "    #                 x = len(df_x)\n",
    "    # #                 y = len(df[(df[seg_y]==seg)])\n",
    "    #                 z = len(df.loc[df[output + '_change_value' + year].dropna().index])\n",
    "    #                 y = len(df_seg)\n",
    "                    high_df = df_seg[df_seg[output + '_change_value' + year].apply(lambda x : x >= thres)]\n",
    "    #                 high_change = len(high_df)\n",
    "    #                 incr = len(df[(df[seg_y]==seg) & (df[output + '_increase'+year]==True) & (df[output + '_change_value' + year] > thres)])\n",
    "    #                 decr = len(df[(df[seg_y]==seg) & (df[output + '_decrease'+year]==True) & (df[output + '_change_value' + year] < -1.0*thres)])\n",
    "    #     #           print(df_x['segment_crop_sales___output_inversion'+year])\n",
    "    #                 c_mean = np.mean(df_seg[output + '_change_value' + year].as_matrix())\n",
    "    #                 c_stddev = np.std(df_seg[output + '_change_value' + year].as_matrix())\n",
    "    #                 a = len(high_df[(high_df[output + '_inversion'+year]>=0)])\n",
    "    #                 b = len(high_df[(high_df[output + '_inversion'+year]<0)])\n",
    "                    exp_y_i.append(len(high_df[(high_df[imp_feat + '_increase'+year]==True)]))\n",
    "                    exp_y.append(len(high_df))\n",
    "                    exp.append(len(df_seg))\n",
    "                    exp_i.append(len(df_seg[df_seg[imp_feat + '_increase'+year]==True]))\n",
    "                    avg_y_i = np.mean(high_df[(high_df[imp_feat + '_increase'+year]==True)][output + '_change_value' + year])\n",
    "                    avg_y = np.mean(high_df[output + '_change_value' + year].dropna())\n",
    "                    avg = np.median(df[output + '_change_value' + year].dropna())\n",
    "                row = {}\n",
    "                row['threshold'] = per\n",
    "                row['input'] = imp_feat\n",
    "#                 row['from'] = year\n",
    "#                 row['to'] = y2\n",
    "                row['cluster'] = seg\n",
    "                row['movement overall'] = (sum(exp_y)/sum(exp))*100.0\n",
    "                row['movement conditioned'] = (sum(exp_y_i)/sum(exp_i))*100.0\n",
    "                row['movement lift'] = (sum(exp_y_i)/sum(exp_i))/(sum(exp_y)/sum(exp))\n",
    "#                 row['raw lift'] = avg_y_i/avg_y\n",
    "                new_row = pd.DataFrame(row, index=[0])\n",
    "                series = series.append(new_row, ignore_index=True)\n",
    "\n",
    "series.to_csv('./threshold-lift.csv')\n",
    "    #                 print (year, seg, np.percentile(df_seg[output + '_change_value' + year].dropna(), 75))\n",
    "    #                 _i = np.mean(df_seg[df_seg[imp_feat + '_increase'+year]==True][output + '_change_value' + year])\n",
    "    #                 print (imp_feat, year, y2, seg, (a/(a+b))*100.0, x/y*100.0, high_change/y*100.0, incr/y*100.0, decr/y*100.0, y/z*100.0, y, x, a, c_mean, c_stddev)\n",
    "#                     print (per, imp_feat, year, y2, seg, exp_i, (exp_y/exp)*100.0, (exp_y_i/exp_i)*100.0, (exp_y_i/exp_i)/(exp_y/exp), avg_y_i/avg_y) \n",
    "                \n",
    "#                 seg_y = for_year(output, year)\n",
    "#                 df_x = imp_df[(df[seg_y]==seg) & (df[output + '_change'+year]==True)]\n",
    "#                 incr = sum(imp_df[(df[seg_y]==seg) & (df[output + '_increase'+year]==True)][weight])\n",
    "#                 decr = sum(imp_df[(df[seg_y]==seg) & (df[output + '_decrease'+year]==True)][weight])\n",
    "#                 x = sum(df_x[weight])\n",
    "#                 y = sum(imp_df[(df[seg_y]==seg)][weight])\n",
    "#                 z = sum(imp_df[weight])\n",
    "#                 df_seg = df[df[seg_y]==seg]\n",
    "#                 df_seg = df_seg.loc[df_seg[output + '_change_value' + year].dropna().index]\n",
    "#                 imp_seg = imp_df.loc[df[df[seg_y]==seg][output + '_change_value' + year].dropna().index]\n",
    "#     #             print(df_x['segment_crop_sales___output_inversion'+year])\n",
    "#                 thres = 1.0\n",
    "#                 high_change = len(df_seg[output + '_change_value' + year].abs().apply(lambda x : x > thres))\n",
    "#                 values = df_seg[output + '_change_value' + year].as_matrix()\n",
    "#                 c_mean = np.average(values, weights=imp_seg[weight])\n",
    "#                 average = c_mean\n",
    "#                 variance = np.average((values-average)**2, weights=imp_seg[weight])\n",
    "#                 c_stddev = math.sqrt(variance)/math.sqrt(len(values))\n",
    "#                 a = sum(imp_df[(df[seg_y]==seg) & (df[output + '_change'+year]==True) & (df[output + '_inversion'+year]>=0)][weight])\n",
    "#                 b = sum(imp_df[(df[seg_y]==seg) & (df[output + '_change'+year]==True) & (df[output + '_inversion'+year]<0)][weight])\n",
    "#                 print (imp_feat, year, y2, seg, (a/(a+b))*100.0, x/y*100.0, high_change/y*100.0, incr/y*100.0, decr/y*100.0, y/z*100.0, c_mean, c_stddev)\n",
    "\n",
    "# df['any_change']= False\n",
    "# for output in outputs:\n",
    "#     df['any_change'] |= (df[output+'_change']==True)\n",
    "\n",
    "\n",
    "# a = len(df[(df['segment_crop_sales___output_change2011']==True) & (df['segment_crop_sales___output_inversion2011']>=0)])\n",
    "# b = len(df[(df['segment_crop_sales___output_change2011']==True) & (df['segment_crop_sales___output_inversion2011']<0)])\n",
    "# c = len(df[(df['segment_crop_sales___output_change2013']==True) & (df['segment_crop_sales___output_inversion2013']>=0)])\n",
    "# d = len(df[(df['segment_crop_sales___outavg_yput_change2013']==True) & (df['segment_crop_sales___output_inversion2013']<0)])\n",
    "# [a/(a+b), c/(c+d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.7433489827856025\n",
      "8.450704225352112\n",
      "29.42097026604069\n",
      "24.49139280125196\n",
      "3.7167449139280127\n",
      "33.17683881064163\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    print(len(df[df['segment_crop_sales___output___ethiopia_2011'] == i])/len(df)*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.596042868920033"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-len(df[df['cluster2011']==0])/len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9280125195618153"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2372/(2372+184)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
       "            ...\n",
       "            3628, 3629, 3630, 3631, 3632, 3633, 3635, 3636, 3637, 3638],\n",
       "           dtype='int64', length=3608)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_1_2011 = a/(a+b)\n",
    "t_1_2013 = c/(c+d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-9faf8e73a38b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
